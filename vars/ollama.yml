ollama_namespace: ollama
ollama_host_name: ollama # alias of "ingress"

# remember to add ollama.fourteeners.local to pfSense DNS
# as an alias of ingress.fourteeners.local: 192.168.4.222
# https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#products
ollama_fqdn: "{{ [ollama_host_name] | product(search_domains) | map('join','.') | first }}"

ollama_service_host: "{{ ollama_release_name }}.{{ ollama_namespace }}.svc.{{ cluster_domain }}"
ollama_service_port: 11434
ollama_service_url: http://{{ ollama_service_host }}:{{ ollama_service_port }}

ollama_secrets:
  ingress: ollama-tls

# "cowboysysop" or "otwld"
use_ollama_chart_by: otwld
ollama_release_name: ollama

# increase Ollama default
# 2048-token context size
ollama_context_size: "{{ 32 * 1024 }}"

# Helm chart values used by both
# cowboysysop and otwld versions
ollama_common_values:
  # run Ollama on a GPU-capable node based
  # on label set by Node Feature Discovery
  # (regardless if a GPU is actually used)
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: intel.feature.node.kubernetes.io/gpu
                operator: Exists # Intel GPU
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 40
          preference:
            matchExpressions:
              # custom label to select latest generation GPU
              - key: feature.node.kubernetes.io/gpu.preferred
                operator: Exists
        - weight: 60
          preference:
            matchExpressions:
              # label defined in vars/kubernetes.yml
              - key: node-role.kubernetes.io/runner
                operator: Exists

  resources:
    requests:
      cpu: 500m
      memory: 3Gi
      gpu.intel.com/i915: "1"
    limits:
      cpu: 4000m
      memory: 6Gi # should be enough for model
      gpu.intel.com/i915: "1"

  storage:
    # use emptyDir instead of PVC because
    # we'll use NFS mount to store models
    enabled: false
    # don't need replicas of huge model files!
    storageClass: "{{ storage_classes['single'] }}"
    size: 4Gi # should be enough for two models

  extraVolumes:
    - name: ollama-nfs
      nfs:
        server: qnap.{{ homelab_domain }}
        path: /k8s_data/ollama

  podSecurityContext:
    fsGroup: 2000
  securityContext:
    capabilities:
      drop: ["ALL"]
    # allow Ollama to create
    #  ~/.ollama/id_ed25519
    readOnlyRootFilesystem: false
    runAsNonRoot: true
    runAsUser: 1000

  readinessProbe: &probe
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 6
  livenessProbe: *probe

  ingress:
    tls:
      - secretName: "{{ ollama_secrets['ingress'] }}"
        hosts: "{{ [ollama_fqdn] }}"
    annotations:
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
      # https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations#custom-timeouts
      nginx.ingress.kubernetes.io/proxy-read-timeout: 10m

  # create LoadBalancer service to listen on non-TLS
  # port (OLLAMA_HOST=ollama.fourteeners.local:11434)
  # in addition to ingress controller on default HTTPS
  # port (OLLAMA_HOST=https://ollama.fourteeners.local)
  externalService:
    apiVersion: v1
    kind: Service
    metadata:
      name: "{{ ollama_release_name }}-external"
      labels:
        app.kubernetes.io/name: ollama
        app.kubernetes.io/instance: "{{ ollama_release_name }}"
      annotations:
        # replaces deprecated .spec.loadBalancerIP field
        metallb.universe.tf/loadBalancerIPs: "{{ metallb_vips['ollama'] }}"
    spec:
      type: LoadBalancer
      ports:
        - name: http
          port: "{{ ollama_service_port }}"
          protocol: TCP
          targetPort: http
          appProtocol: http
      selector:
        app.kubernetes.io/name: ollama
        app.kubernetes.io/instance: "{{ ollama_release_name }}"
