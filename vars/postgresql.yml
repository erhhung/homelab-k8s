# postgresql_pass: {vault.yml}
#     pgpool_pass: {vault.yml}

# in order to log in using client TLS cert, generate
# a cert at pki.fourteeners.local using CN=<db_user>
# IMPORTANT: users (roles) are only created when the
# cluster is initially deployed--new users will have
# to be created manually afterwards
postgresql_users:
  - erhhung
  - scraper
  - "{{  keycloak_db_user }}"
  - "{{    gitlab_db_user }}"
  - "{{ openwebui_db_user }}"
  - "{{   flowise_db_user }}"

postgresql_superuser: erhhung

postgresql_namespace: postgresql
postgresql_host_names: # 192.168.4.224
  - postgresql
  - postgres
  - pg

# remember to add postgresql.fourteeners.local (also postgres.
# + pg.) to pfSense DNS using MetalLB virtual IP 192.168.4.224
# https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#products
postgresql_fqdns: "{{ postgresql_host_names | product(search_domains) | map('join','.') }}"

pgpool_service_host: "{{ postgresql_release_name }}-pgpool.{{ postgresql_namespace }}.svc.{{ cluster_domain }}"
pgpool_service_port: 5432
pgpool_service_url: postgresql://{{ pgpool_service_host }}:{{ pgpool_service_port }}

postgresql_secrets:
  node-tls: postgresql-node-tls
  # this secret isn't created in the PostgreSQL
  # namespace--it's created in other namespaces
  # to enable installation of custom extensions
  # that require superuser privileges
  superuser: database-superuser

# https://github.com/bitnami/charts/tree/main/bitnami/postgresql-ha
postgresql_chart_version: "16.2.2"
postgresql_release_name: postgresql

# https://github.com/bitnami/charts/tree/main/bitnami/postgresql-ha/values.yaml
postgresql_chart_values:
  nameOverride: "{{ postgresql_release_name }}"

  global:
    defaultStorageClass: "{{ storage_classes['default'] }}"
    postgresql: &global_postgresql
      password: "{{ postgresql_pass }}" # user "postgres"
      repmgrPassword: "{{ postgresql_pass }}" # user "repmgr"
    pgpool: &global_pgpool
      adminPassword: "{{ postgresql_pass }}" # user "admin"

  persistence:
    size: 5Gi
    # don't change the mountPath because at least one
    # postgresql container volumeMount hardcodes this
    mountPath: /bitnami/postgresql

  volumePermissions:
    # required when providing
    # postgresql.extendedConf
    enabled: true

  postgresql:
    replicaCount: 3

    # duplicate global settings or
    # else helm diff upgrade fails
    <<: *global_postgresql
    pgHbaTrustAll: true

    # https://github.com/bitnami/charts/tree/main/bitnami/postgresql-ha#securing-traffic-using-tls
    tls: &node_tls
      enabled: true
      certificatesSecret: "{{ postgresql_secrets['node-tls'] }}"
      certFilename: tls.crt
      certKeyFilename: tls.key
      certCAFilename: ca.crt

    # resourcesPreset: small
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
        ephemeral-storage: 50Mi

    priorityClassName: "{{ priority_classes['database'] }}"

    readinessProbe:
      enabled: false
    # chart-defined probe
    # is too complicated!
    customLivenessProbe:
      exec:
        command:
          - sh
          - -c
          - test "$(psql -U postgres -c "SELECT 1" -tAq)" = 1
      initialDelaySeconds: 30
      periodSeconds: 20
      timeoutSeconds: 5
      successThreshold: 1
      failureThreshold: 6

    # syncReplication sets synchronous_commit = 'on', but
    # we override to 'remote_apply' in extendedConf below
    syncReplication: true
    # https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-SYNCHRONOUS-STANDBY-NAMES
    syncReplicationMode: ANY

    # similar to postgresql.configuration
    # but appended to  main configuration:
    # /bitnami/postgresql/conf/postgresql.conf
    # /bitnami/postgresql/conf/conf.d/override.conf
    extendedConf: |
      # this setting could drastically increase write latency but will guarantee
      # consistency since reads could come from any load-balanced Pgpool replica:
      # https://www.postgresql.org/docs/current/runtime-config-wal.html#GUC-SYNCHRONOUS-COMMIT
      synchronous_commit = 'remote_apply'

    maxConnections: 5000 # max total connections
    postgresConnectionLimit: 200 # max connections by 'postgres' user
    dbUserConnectionLimit: 1000 # max connections by {username} user

    sharedPreloadLibraries: pgaudit # CSV

    # https://github.com/bitnami/charts/tree/main/bitnami/postgresql-ha#initialize-a-fresh-instance
    initdbScripts:
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/template_lookup.html
      init.sql: "{{ lookup('ansible.builtin.template', template_dir ~ '/postgresql/init.sql.j2') }}"

  pgpool:
    replicaCount: 2

    # duplicate global settings or
    # else helm diff upgrade fails
    <<: *global_pgpool

    # use same certificate as postgresql:
    # https://github.com/bitnami/charts/tree/main/bitnami/postgresql-ha#securing-traffic-using-tls
    tls: *node_tls

    # https://github.com/bitnami/charts/tree/main/bitnami/postgresql-ha#resource-requests-and-limits
    # use a preset instead of explicitly configuring resources:
    # values: none|nano|micro|small|medium|large|xlarge|2xlarge
    # resourcesPreset: micro
    resources:
      requests:
        cpu: 50m
        memory: 512Mi

    priorityClassName: "{{ priority_classes['database'] }}"

    startupProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 5
      successThreshold: 1
      failureThreshold: 12
    readinessProbe:
      enabled: true
      initialDelaySeconds: 10
      periodSeconds: 30
      timeoutSeconds: 15
      successThreshold: 1
      failureThreshold: 10
    livenessProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 60
      timeoutSeconds: 15
      successThreshold: 1
      failureThreshold: 10

    # Pgpool Control Protocol
    logPcpProcesses: false

    # this is off only because synchronous_commit = 'remote_apply'
    # in postgresql.conf to ensure consistency by trading latency:
    # https://www.pgpool.net/docs/latest/en/html/runtime-config-load-balancing.html#RUNTIME-CONFIG-LOAD-BALANCING-SETTINGS
    disableLoadBalancingOnWrite: "off" # must be quoted

    # num_init_children = number of preforked Pgpool server
    # processes, or the concurrent client connections limit
    numInitChildren: 500
    # number of connections Pgpool will always allow beyond
    # (num_init_children - reserved_connections) concurrent
    # connections by superusers like "postgres"
    reservedConnections: 50
    # maximum number of cached connections (combination
    # of specific user and database) in a child process
    maxPool: 4
    # number of sequential connections each child process
    # can serve before it gets respawned (prevents memory
    # leaks in long-running processes)
    childMaxConnections: 500

    customUsers:
      # semicolon-separated list of usernames & passwords
      # of users that will connect to database via pgpool
      usernames: "{{ postgresql_users | join(';') }}"
      passwords: |-
        {% set pwds = [] -%}
        {% for _ in postgresql_users -%}
        {%   set _ = pwds.append(pgpool_pass) -%}
        {% endfor -%}
        {{ pwds | join(';') }}

    initContainers:
      # make sure PgPool doesn't probe PostgreSQL nodes too
      # early, which can cause failures like "Startup probe
      # failed: Found inconsistencies in pgpool_status" but
      # cluster nodes may still require StatefulSet rollout
      # restart after underlying VMs started simultaneously
      - name: wait-postgresql
        image: '{% raw %}{{ include "postgresql-ha.postgresql.image" . }}{% endraw %}'
        imagePullPolicy: IfNotPresent
        command: ["bash"]
        args:
          - -c
          - |-
            set -eo pipefail

            # wait for DNS + connections
            hosts=(${BACKEND_HOSTS//,/ })

            for host in "${hosts[@]}"; do
              echo   "Waiting for $host to accept connections..."
              until  getent hosts $host &> /dev/null; do sleep 1; done
              until pg_isready -h $host &> /dev/null; do sleep 1; done
              echo  OK
            done

            # wait for stable topology:
            # 1 primary + n-1 replicas
            tries=0

            while true; do
              echo "Waiting for a stable role topology..."
               primary=0
              replicas=0

              for host in "${hosts[@]}"; do
                role=$(psql -h $host -tAc "SELECT CASE WHEN pg_is_in_recovery() THEN 'r' ELSE 'p' END")
                case "$role" in
                  p)  ((primary++)) || true ;;
                  r) ((replicas++)) || true ;;
                esac
              done

              if [ $primary  -eq 1 ] && \
            {%- raw %}
                 [ $replicas -eq $(( ${#hosts[@]} - 1 )) ]; then
            {% endraw %}
                echo "Topology stable: 1 primary, $replicas replicas"
                break
              fi

              ((tries++))  || true
              if [ $tries -gt 60 ]; then
                echo >&2 "Timed out waiting for stable topology."
                exit 1
              fi
              sleep 2
            done
        env:
          - name: BACKEND_HOSTS
            # use full FQDNs of the StatefulSet
            # pods backing the headless service
            value: |-
              {% set hosts = [] -%}
              {% for i in range(postgresql_chart_values.postgresql.replicaCount) -%}
              {%   set _ = hosts.append(postgresql_release_name ~ '-postgresql-' ~i~'.' ~
                                        postgresql_release_name ~ '-postgresql-headless.' ~
                                        postgresql_namespace    ~ '.svc.' ~ cluster_domain) -%}
              {% endfor -%}
              {{ hosts  | join(',') }}
          - name: PGUSER
            # NOTE: pgoolSrCheckUsername is misspelled in the Helm chart _helpers.tpl!
            value: '{% raw %}{{ include "postgresql-ha.pgoolSrCheckUsername" . }}{% endraw %}'
          - name: PGPASSWORD
            valueFrom:
              secretKeyRef:
                name: '{% raw %}{{ include "postgresql-ha.pgpoolSecretName" . }}{% endraw %}'
                key: sr-check-password
          - name: PGDATABASE
            value: '{% raw %}{{ .Values.pgpool.srCheckDatabase }}{% endraw %}'

    sidecars:
      # https://github.com/pgpool/pgpool2_exporter
      # currently disabled because pgpool-exporter
      # cannot connect to Pgpool endpoint via mTLS:
      # https://github.com/pgpool/pgpool2_exporter/issues/43

      # - name: pgpool-exporter
      #   image: docker.io/pgpool/pgpool2_exporter
      #   imagePullPolicy: IfNotPresent
      #   env:
      #     - name: POSTGRES_USERNAME
      #       value: scraper
      #     - name: POSTGRES_PASSWORD
      #       valueFrom:
      #         secretKeyRef:
      #           name: postgresql-postgresql
      #           key: password
      #     - name: POSTGRES_DATABASE
      #       value: postgres
      #     - name: PGPOOL_SERVICE
      #       value: localhost
      #     - name: PGPOOL_SERVICE_PORT
      #       value: "{{ pgpool_service_port }}"
      #     - name: SSLMODE
      #       value: verify-full
      #   ports:
      #     # exposed by service in extraDeploy
      #     - name: metrics
      #       containerPort: 9719
      #       protocol: TCP
      #   securityContext:
      #     runAsUser: 1001
      #     runAsGroup: 1001
      #     capabilities:
      #       drop: ["ALL"]
      #   volumeMounts:
      #     - name: scraper-tls
      #       mountPath: /tls/scraper

    extraVolumes:
      - name: scraper-tls
        secret:
          secretName: "{{ monitoring_secrets['scraper'] }}"

  service: # port 5432
    # since PostgreSQL does not talk HTTP, we do not create
    # an Ingress; instead, we create a LoadBalancer service:
    # https://github.com/bitnami/charts/tree/main/bitnami/postgresql-ha#configure-the-way-how-to-expose-postgresql
    type: LoadBalancer

    annotations:
      # replaces deprecated .spec.loadBalancerIP field
      metallb.universe.tf/loadBalancerIPs: "{{ metallb_vips['postgresql'] }}"

    sessionAffinity: ClientIP
    sessionAffinityConfig:
      clientIP:
        timeoutSeconds: 28800 # 8 hours

  metrics:
    # adds postgres-exporter sidecar container:
    # https://github.com/bitnami/charts/tree/main/bitnami/postgresql-ha#prometheus-metrics
    # https://github.com/prometheus-community/postgres_exporter
    enabled: true

    # scrape dedicated postgresql-metrics
    # service on HTTP "metrics" port 9187
    serviceMonitor:
      enabled: "{{ prometheus_crds_installed }}"
      labels: &smon-labels
        release: "{{ monitoring_release_name }}"
      selector: *smon-labels

  # NOTE: Helm will render these manifests as templates,
  # but the only templating currently used are processed
  # first by Ansible
  extraDeploy:
    # the following resources are currently disabled
    # because pgpool-exporter sidecar cannot connect
    # to Pgpool endpoint via mTLS:
    # https://github.com/pgpool/pgpool2_exporter/issues/43
    - |-
      {% if prometheus_crds_installed and false %}
      # create a service to expose metrics endpoint from
      # pgpool-exporter sidecar of postgresql-pgpool pod
      #
      apiVersion: v1
      kind: Service
      metadata:
        name: {{ postgresql_release_name }}-pgpool-metrics
        labels:
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/component: pgpool
          app.kubernetes.io/instance: {{ postgresql_release_name }}
          release: {{ monitoring_release_name }}
      spec:
        type: ClusterIP
        selector:
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/component: pgpool
          app.kubernetes.io/instance: {{ postgresql_release_name }}
        ports:
          - name: metrics
            port: 9719
            protocol: TCP
            targetPort: metrics
      ---
      apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      metadata:
        name: {{ postgresql_release_name }}-pgpool
        labels:
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/component: pgpool
          app.kubernetes.io/instance: {{ postgresql_release_name }}
          # required by serviceMonitorSelector!
          release: {{ monitoring_release_name }}
      spec:
        namespaceSelector:
          matchNames:
            - {{ postgresql_namespace }}
        selector:
          matchLabels:
            app.kubernetes.io/name: postgresql
            app.kubernetes.io/component: pgpool
            release: {{ monitoring_release_name }}
        endpoints:
          - port: metrics
            path: /metrics
            interval: 30s
      {% endif %}

# https://grafana.com/grafana/dashboards/?search=PostgreSQL&dataSource=prometheus
postgresql_grafana_dashboards:
  # https://grafana.com/grafana/dashboards/9628-postgresql-database
  - title: PostgreSQL Database
    gnet_id: 9628
