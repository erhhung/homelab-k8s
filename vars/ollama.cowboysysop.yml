# https://github.com/cowboysysop/charts/tree/master/charts/ollama
ollama_chart_repo: https://cowboysysop.github.io/charts
ollama_chart_version: "2.1.0"

# https://github.com/cowboysysop/charts/tree/master/charts/ollama/values.yaml
ollama_chart_values:
  replicaCount: 1
  revisionHistoryLimit: 2

  affinity: "{{ ollama_common_values['affinity'] }}"
  resources: "{{ ollama_common_values['resources'] }}"
  persistence: "{{ ollama_common_values['storage'] }}"

  image:
    pullPolicy: Always
    # use custom Docker image (files/ollama/Dockerfile) that
    # runs "Ollama Portable Zip" on Intel GPU with IPEX-LLM:
    # https://github.com/intel/ipex-llm/tree/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md
    registry: "{{ harbor_container_registry }}"
    repository: library/ollama-ipex-llm
    tag: latest

  # Environment variables: ollama serve --help
  # https://github.com/ollama/ollama/tree/main/envconfig/config.go
  # https://github.com/ollama/ollama/issues/2941#issuecomment-2322778733
  extraEnvVars:
    # run Gin in production mode
    - name: GIN_MODE
      value: release
    # enable Intel GPU detection
    - name: OLLAMA_INTEL_GPU
      value: "1"
    # force all layers to use GPU
    # (IPEX-LLM-specific env var)
    - name: OLLAMA_NUM_GPU
      value: "999"
    # force Ollama to use one GPU
    - name: ONEAPI_DEVICE_SELECTOR
      value: level_zero:0
    # limit to 1 request at a time so
    # cluster doesn't grind to a halt!
    - name: OLLAMA_NUM_PARALLEL
      value: "1"
    # memory limit only holds 1 model
    - name: OLLAMA_MAX_LOADED_MODELS
      value: "1"
    # fail model loading after
    - name: OLLAMA_LOAD_TIMEOUT
      value: 15m
    # keep model in memory for
    - name: OLLAMA_KEEP_ALIVE
      value: 30m
    # increase Ollama default
    # 2048-token context size
    - name: OLLAMA_CONTEXT_LENGTH
      value: &num_ctx >-
        {{ ollama_context_size }}
    # version < 2.3.0b20250429
    - name: IPEX_LLM_NUM_CTX
      value: *num_ctx
    # per IPEX-LLM recommendation
    - name: SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS
      value: "1"
    # per llama.cpp recommendation
    - name: SYCL_CACHE_PERSISTENT
      value: "1"
    # support ext_intel_free_memory
    - name: ZES_ENABLE_SYSMAN
      value: "1"

  podSecurityContext: "{{ ollama_common_values['podSecurityContext'] }}"
  securityContext: "{{ ollama_common_values['securityContext'] }}"

  readinessProbe: "{{ ollama_common_values['readinessProbe'] }}"
  livenessProbe: "{{ ollama_common_values['livenessProbe'] }}"

  extraVolumes: "{{ ollama_common_values['extraVolumes'] }}"
  extraVolumeMounts:
    - name: ollama-nfs
      subPath: models
      # chart already mounts /data and sets OLLAMA_MODELS
      # to /data/models, so shadow that path to NFS mount
      mountPath: /data/models

  service:
    type: ClusterIP
    ports:
      http: "{{ ollama_service_port }}"

  ingress:
    enabled: true
    ingressClassName: "{{ rke_ingress_class }}"
    annotations: "{{ ollama_common_values['ingress'].annotations }}"
    tls: "{{ ollama_common_values['ingress'].tls }}"
    hosts:
      - host: "{{ ollama_fqdn }}"
        paths: ["/"]
    pathType: Prefix

  extraDeploy:
    - "{{ ollama_common_values['externalService'] }}"
    # create Job to pull models on container startup
    - |
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: {{ ollama_release_name }}-models
        labels:
          app.kubernetes.io/name: ollama
          app.kubernetes.io/instance: {{ ollama_release_name }}
      spec:
        template:
          spec:
            containers:
              - name: ollama
                # use official image as client
                image: docker.io/ollama/ollama
                imagePullPolicy: IfNotPresent
                command:
                  - bash
                args:
                  - -c
                  - |
                    (apt-get update; apt-get install curl -y) &> /dev/null
                    until curl -s $OLLAMA_HOST; do
                      echo 'Waiting for Ollama...'
                      sleep 5
                    done
                    echo -e '!\n\nPulling models...'
                    for model in {{ ollama_models | join(' ') }}; do
                      ollama pull $model || exit $?
                    done
                env:
                  - name: OLLAMA_HOST
                    value: {{ ollama_release_name }}:{{ ollama_service_port }}
            restartPolicy: OnFailure

        # allow 5 minutes per model
        activeDeadlineSeconds: {{ 300 * ollama_models | length }}
        backoffLimit: 2 # retry twice
        # keep pod around for an hour
        ttlSecondsAfterFinished: 3600

# models to pull on startup
# https://ollama.com/search
ollama_models:
  - gemma2:2b # 1.6GB
  - llama3.2:3b # 2.0GB
  #- qwen2.5:7b # 4.7GB

  # NOTE: the latest "Ollama Portable Zip" from
  # IPEX-LLM is based on an Ollama version that
  # is too old to run some of the latest models
  #- deepseek-r1:8b # 5.2GB
