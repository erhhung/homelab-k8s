data_lv:
  lv: data-lv # name of logical volume
  vg: ubuntu-vg # name of volume group
  fs: xfs # volume filesystem type
  # unit capitalization: g=GB and G=GiB
  size: 95%FREE # size of logical volume
  mount: /data # path to mount volume

storage_classes:
  default: longhorn # 2 replicas
  single: longhorn-single # 1 replica
  static: longhorn-static
  local: local-storage
  nfs: nfs-qnap

snapshot_classes:
  backup: longhorn-backup

# cannot use any other namespace!
longhorn_namespace: longhorn-system

longhorn_secrets:
  minio: minio-credentials

# https://github.com/longhorn/charts/tree/master/charts/longhorn
longhorn_chart_version: "1.9.1"
longhorn_release_name: longhorn

# https://github.com/longhorn/charts/tree/v1.9.x/charts/longhorn/values.yaml
# https://longhorn.io/docs/1.9.0/references/settings
longhorn_chart_values:
  longhornUI:
    replicas: 1

  persistence:
    defaultFsType: xfs
    # total copies of data
    defaultClassReplicaCount: 2
    # what happens when PVC is released
    reclaimPolicy: Delete
    # allow live migration of Longhorn
    # volumes from one node to another
    # migratable: true

  defaultSettings:
    defaultDataPath: "{{ data_lv.mount }}"
    # prefer replicas of the same volume to be
    # on different nodes, but allow them to be
    # on the same node should space be limited
    replicaSoftAntiAffinity: true
    defaultReplicaCount: 2
    # Velero is configured to perform daily
    # backups with volume snapshot TTL of 3
    # days, so set snapshotMaxCount to >= 3
    snapshotMaxCount: 6

  defaultBackupStore:
    # user "longhorn" and "backups"
    # bucket to be created by MinIO
    backupTarget: s3://backups@{{ minio_region }}/longhorn
    backupTargetCredentialSecret: "{{ longhorn_secrets['minio'] }}"

  csi:
    # one extra replica should be enough
    # for all controllers (default is 3)
    provisionerReplicaCount: 2
    attacherReplicaCount: 2
    resizerReplicaCount: 2
    snapshotterReplicaCount: 2

  service:
    ui:
      type: Rancher-Proxy

  ingress:
    # access Longhorn UI via Rancher Server
    # console instead (see service.ui.type)
    enabled: false

  metrics:
    # scrapes longhorn-backend service (longhorn-manager
    # DaemonSet) on port 9500 (non-TLS) on path /metrics
    serviceMonitor:
      enabled: "{{ prometheus_crds_installed }}"
      additionalLabels:
        release: "{{ monitoring_release_name }}"

  extraObjects:
    # create VolumeSnapshotClass for remote backups (use
    # "bak" as parameters.type instead of "snap" because
    # in-cluster snapshots are useless in a DR scenario):
    # https://longhorn.io/docs/latest/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup
    - apiVersion: snapshot.storage.k8s.io/v1
      kind: VolumeSnapshotClass
      metadata:
        name: "{{ snapshot_classes['backup'] }}"
        labels:
          # make Velero create VSs using this class:
          # https://velero.io/docs/main/csi#implementation-choices
          velero.io/csi-volumesnapshot-class: "true"
      driver: driver.longhorn.io
      deletionPolicy: Delete
      parameters:
        type: bak
        # full backup allows simple n-day deletion
        # policy without worrying about incremental
        # backup version chain dependencies
        backupMode: full # incremental|full

    # create daily CronJob to delete old Longhorn backups
    # (set cutoff to 3 days to match Velero Schedule TTL)
    - apiVersion: batch/v1
      kind: CronJob
      metadata:
        name: delete-backups
      spec:
        schedule: 0 11 * * * # in UTC time; run every day at 3am Pacific time
        concurrencyPolicy: Forbid
        jobTemplate:
          spec:
            template:
              spec:
                # use pre-defined SA with delete permissions
                serviceAccountName: longhorn-service-account
                restartPolicy: OnFailure
                containers:
                  - name: delete-backups
                    image: "{{ harbor_container_registry }}/library/al2023-devops:latest"
                    imagePullPolicy: IfNotPresent
                    command: ["bash"]
                    args:
                      - -c
                      - |-
                        THUMBUP=$'\U1F44D'

                        # print given title with
                        # another line of dashes
                        title() {
                          local s="$*" d l t
                          echo -e "$s"

                          # use PCRE (*SKIP)(*F) trick to preserve ANSI
                          # color codes while replacing all other chars
                          d="$(perl -pe 's/(?:\\(033|x1B)\[[0-9;]*[mGKHF])(*SKIP)(*F)|./-/g' <<< "$s")"

                          #  remove  leading and trailing dashes
                          # matching leading and trailing spaces
                          l="${s%%[^[:space:]]*}" #  leading spaces
                          t="${s##*[^[:space:]]}" # trailing spaces
                          {% raw -%}
                          echo -e "$l${d:${#l}:${#d}-${#l}-${#t}}$t"
                          {%- endraw +%}
                        }

                        # mark start time
                        tspush () {
                          _timers+=($EPOCHREALTIME)
                        }

                        # end marked time
                        # and set _tstart
                        tspop () {
                          unset _tstart
                          {% raw -%}
                          local i=$(( ${#_timers[@]}-1 ))
                          {%- endraw +%}
                          [ $i -ge 0 ] || return 1

                          _tstart=${_timers[$i]}
                          _timers=(${_timers[@]:0:$i})
                        }

                        # show time elapsed since last mark
                        # [decimals=3] [label="DURATION:"]
                        elapsed() {
                          local t1 t0 lbl sec dec int hms
                          t1=$EPOCHREALTIME
                          tspop  && t0=$_tstart
                          [ "$t0" ] || return 1

                          dec=${1:-3}; int=$((dec+2))
                          [ $dec -gt 0 ] && ((int++))
                          shift; lbl="${*:-DURATION:}"
                          # make sure values < 1.0 have leading
                          # zero so hms computation below works
                          sec="0$(echo "$t1 - $t0" | bc)"

                          printf -v hms "%02d:%02d:%0${int}.${dec}f"   \
                            $((${sec%.*}/3600)) $(((${sec%.*}/60)%60)) \
                            $(bc <<< "$sec%60")
                          echo "$lbl $hms"
                        }

                        num=0; tspush # measure total job time
                        while read -r name date total pvc mode size; do
                          echo; title DELETING $((++num))/$total: $name
                          tspush # measure per item time

                          # display ISO 8601 UTC date in local time zone
                          date=$(TZ="{{ system_time_zone }}"  date -d $date "+%Y-%m-%d %H:%M:%S %Z")
                          printf "Date: %s\n PVC: %s\nMode: %s\nSize: %d\n" "$date" $pvc $mode $size
                          kubectl delete backup.longhorn.io $name -n {{ longhorn_namespace }}

                          elapsed 3 DELETE TIME:
                        done < <(
                          kubectl get backups.longhorn.io -n {{ longhorn_namespace }} -o json  | \
                            jq --arg cutoff $(date -d "-3 days" --iso-8601=seconds) -r '.items |
                              map(select(
                                .status.labels.KubernetesStatus and
                                .status.backupCreatedAt < $cutoff)
                              ) | sort_by(.status.backupCreatedAt)
                                | length as $total | .[] |
                              (.status.labels.KubernetesStatus | fromjson) as $pv
                                | "\(.metadata.name) \(.status.backupCreatedAt) "
                                + "\($total) \($pv.namespace)/\($pv.pvcName) "
                                + "\(.spec.backupMode) \(.status.size)"'
                        )
                        [ $num -gt 0 ] && echo
                        echo "Deleted $num Longhorn backups."
                        elapsed 2 $THUMBUP TOTAL TIME:
            # actual storage deletion will occur asynchronously;
            # `kubectl delete backup` should just mark deletion
            activeDeadlineSeconds: 7200 # 2 hours
            backoffLimit: 3 # 3 retries
            # allow 12 hours to view logs
            ttlSecondsAfterFinished: 43200
        successfulJobsHistoryLimit: 1

# https://longhorn.io/docs/latest/snapshots-and-backups/scheduling-backups-and-snapshots#set-up-recurring-jobs
longhorn_recurring_jobs:
  - name: delete-snapshots
    spec:
      cron: 0 10 * * * # in UTC time; run every day at 2am Pacific time
      task: snapshot-delete
      groups: ["default"]
      retain: 3
      concurrency: 1

  # Longhorn RecurringJob doesn't currently support
  # a task type for deleting backups, so a CronJob
  # under .extraObjects above is created to delete
  # old backups in the MinIO bucket

# ==================================== NFS Provisioners ====================================

nfs_provisioner_namespace: nfs-provisioner

# https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/tree/master/charts/nfs-subdir-external-provisioner
nfs_provisioner_chart_version: "4.0.18"
# chart values common to all provisioners defined in nfs_provisioners list below:
# https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/tree/master/charts/nfs-subdir-external-provisioner/values.yaml
nfs_provisioner_chart_values:
  resources:
    requests:
      cpu: 10m
      memory: 32Mi

# NOTE: storage class settings cannot be changed
# after creation without deletion and recreation
nfs_provisioners:
  - cluster: k3s
    name: qnap # release name
    server: qnap.{{ homelab_domain }}
    path: /k8s_data/k3s
    storageClass:
      name: "{{ storage_classes['nfs'] }}"
      accessModes: ReadWriteOnce
      volumeBindingMode: Immediate
      reclaimPolicy: Delete
      onDelete: delete

  - cluster: rke
    name: qnap # release name
    server: qnap.{{ homelab_domain }}
    path: /k8s_data/rke
    storageClass:
      name: "{{ storage_classes['nfs'] }}"
      accessModes: ReadWriteOnce
      volumeBindingMode: WaitForFirstConsumer
      reclaimPolicy: Delete
      onDelete: delete

# https://computingforgeeks.com/configure-nfs-as-kubernetes-persistent-volume-storage
# (with recommendations from ChatGPT to better cope with ancient QNAP kernel & NFSv3)
# https://www.man7.org/linux/man-pages/man5/nfs.5.html
nfs_mount_opts:
  - vers=3 # NFSv4 is unreliable on QTS 4.2.x
  - proto=tcp
  - rw
  - nodev
  - noexec
  - nosuid
  - soft # hard would cause indefinite hangs
  - nolock # lockd & statd are too fragile
  - intr # prevents unkillable processes
  - timeo=100 # 10 seconds
  - retrans=5
  - rsize=16384
  - wsize=16384

# =================================== Grafana Dashboards ===================================

# https://grafana.com/grafana/dashboards/?search=Longhorn&dataSource=prometheus
storage_grafana_dashboards:
  # https://grafana.com/grafana/dashboards/22705-longhorn-dashboard
  - title: Longhorn Dashboard
    gnet_id: 22705
    tags:
      - longhorn
      - storage
