# valkey_pass: {vault.yml}

valkey_namespace: valkey

# total nodes = primaries + replicas * primaries
# 6 nodes = 3 primaries + 1 replica per primary
# must have 3+ primaries or cluster init fails
valkey_node_count: 6

# valkey, valkey{1..6}
#  redis,  redis{1..6}
# 192.168.0.22{2..7}
valkey_host_names: |
  {% set hosts = ['valkey', 'redis']   %}
  {% for i in range(valkey_node_count) %}
  {%   set _ = hosts.append('valkey' ~ (i+1)) %}
  {% endfor %}
  {% for i in range(valkey_node_count) %}
  {%   set _ = hosts.append('redis' ~ (i+1)) %}
  {% endfor %}
  {{ hosts  }}

# remember to add valkey1.fourteeners.local thru valkey6.
# (valkey. = <all-vIPs> plus aliases redis1. thru redis6.)
# to pfSense DNS using kube virtual IPs: 192.168.0.222-227
# https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#products
valkey_fqdns: "{{ valkey_host_names | product(search_domains) | map('join','.') }}"

# https://docs.ansible.com/ansible/latest/collections/ansible/utils/ipmath_filter.html
# (requires Python package "netaddr" on Ansible controller)
# ["192.168.0.222", ..."192.168.0.227"]
valkey_lb_ips: |
  {% set ips = [] %}
  {% for i in range(valkey_node_count) %}
  {%   set _ = ips.append(rke_lb_vip | ansible.utils.ipmath(i + 1)) %}
  {% endfor %}
  {{ ips    }}

# ["valkey-cluster-0-svc.valkey.svc.cluster.local", ...]
valkey_node_fqdns: |
  {% set fqdns = [] %}
  {% for i in range(valkey_node_count) %}
  {%   set _ = fqdns.append(valkey_release_name ~'-'~ i ~'-svc.'~
                 valkey_namespace ~'.svc.'~ cluster_domain) %}
  {% endfor %}
  {{ fqdns  }}

# node endpoints: ["{FQDN}:6379"]
valkey_node_eps: "{{ valkey_node_fqdns | product([valkey_service_port | string]) | map('join',':') }}"
# ["rediss://{node-endpoint}", ...]
redis_cluster_urls: "{{ ['rediss://'] | product(valkey_node_eps) | map('join') }}"
# ["valkeys://{node-endpoint}", ...]
valkey_cluster_urls: "{{ ['valkeys://'] | product(valkey_node_eps) | map('join') }}"

# IMPORTANT! client should NOT connect using the Kubernetes
# service endpoint because it load balances each connection,
# lacks strong consistency guarantee due to replication lag
valkey_service_host: "{{ valkey_node_fqdns | first }}"
valkey_service_port: 6379

redis_service_url: "{{ redis_cluster_urls | first }}"
valkey_service_url: "{{ valkey_cluster_urls | first }}"

valkey_secrets:
  node-tls: valkey-node-tls
  password: valkey-password

# https://github.com/bitnami/charts/tree/main/bitnami/valkey-cluster
valkey_chart_version: "3.0.18"
valkey_release_name: valkey

# https://github.com/bitnami/charts/tree/main/bitnami/valkey-cluster/values.yaml
valkey_chart_values:
  nameOverride: "{{ valkey_release_name }}"

  tls: # for replication traffic
    enabled: true
    # IRedis doesn't currently support mTLS parameters
    authClients: false # require nodes to authenticate
    existingSecret: "{{ valkey_secrets['node-tls'] }}"
    certFilename: tls.crt
    certKeyFilename: tls.key
    certCAFilename: ca.crt

  usePassword: true
  existingSecret: "{{ valkey_secrets['password'] }}"
  existingSecretPasswordKey: password

  persistence:
    enabled: true
    # no need for replicas at storage level when
    # the cluster itself stores data in replicas
    storageClass: "{{ storage_classes['single'] }}"
    # min XFS volume size is 300Mi
    size: 384Mi

  persistentVolumeClaimRetentionPolicy:
    enabled: true
    whenScaled: Retain
    whenDeleted: Delete

  valkey:
    # must be yes in quotes!
    useAOFPersistence: "yes"

    # use a preset instead of explicitly configuring resources:
    # none, nano, micro, small, medium, large, xlarge, 2xlarge
    # https://github.com/bitnami/charts/tree/main/bitnami/common/templates/_resources.tpl
    resourcesPreset: micro
    # resources: ~

    priorityClassName: "{{ priority_classes['database'] }}"

    # required for cluster joins
    podManagementPolicy: Parallel
    podAntiAffinityPreset: soft

    containerPorts:
      valkey: 6379
      bus: 16379

  cluster:
    init: true
    nodes: "{{ valkey_node_count }}"
    replicas: 1 # per primary

    externalAccess:
      enabled: true
      service:
        type: LoadBalancer
        port: "{{ valkey_service_port }}"
        # create one service per node and
        # expose on IPs 192.168.0.222-227
        loadBalancerIP: "{{ valkey_lb_ips }}"

  metrics:
    # https://github.com/bitnami/charts/tree/main/bitnami/valkey-cluster#metrics
    enabled: true

    # https://github.com/oliver006/redis_exporter#command-line-flags
    extraArgs:
      is-cluster: "true"
      # no need to verify localhost
      skip-tls-verification: "true"

    serviceMonitor:
      enabled: "{{ prometheus_crds_installed }}"
      labels:
        release: "{{ monitoring_release_name }}"

# https://grafana.com/grafana/dashboards/?search=Valkey&dataSource=prometheus
valkey_grafana_dashboards:
  # https://grafana.com/grafana/dashboards/763-redis-dashboard-for-prometheus-redis-exporter-1-x
  - title: Valkey Dashboard
    gnetId: 763
    dsVar: DS_PROM
