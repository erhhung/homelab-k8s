#   minio_root_pass: {vault.yml}
#  minio_admin_pass: {vault.yml}
# minio_client_pass: {vault.yml}

minio_region: homelab

minio_buckets:
  # bucket for Velero to store backups
  backups: {}
    # https://min.io/docs/minio/linux/reference/minio-mc/mc-mb.html#parameters
    # options: ["--with-versioning"]
  # bucket for Thanos to store metrics
  metrics: {}
  # bucket for Open WebUI
  # to store user uploads
  openwebui: {}

minio_policies:
  # IAM-compatible policies for users and groups
  # only (there is no support for bucket policy)
  # NOTE: rendered JSON doc cannot exceed 20 KiB
  # https://min.io/docs/minio/linux/administration/identity-access-management/policy-based-access-control.html#policy-document-structure
  # lookup_plugins/one_bucket_iam_policy.py creates a broad
  # policy. ideally, allowed actions should be more limited:
  # https://github.com/vmware-tanzu/velero-plugin-for-aws#option-1-set-permissions-with-an-iam-user
  backups-rw: "{{ lookup('one_bucket_iam_policy', 'backups') }}"
  metrics-rw: "{{ lookup('one_bucket_iam_policy', 'metrics') }}"
  openwebui-rw: "{{ lookup('one_bucket_iam_policy', 'openwebui') }}"

minio_users: # non-root
  # name = access_key
  erhhung:
    secret_key: "{{ minio_admin_pass }}"
    # https://min.io/docs/minio/linux/administration/identity-access-management/policy-based-access-control.html#built-in-policies
    # policies: ["readwrite"]
  longhorn:
    # policies managed by "backups" group
    secret_key: "{{ minio_client_pass }}"
  velero:
    # policies managed by "backups" group
    secret_key: "{{ minio_client_pass }}"
  metrics:
    secret_key: "{{ minio_client_pass }}"
    policies: ["diagnostics"]
  thanos:
    # policies managed by "metrics" group
    secret_key: "{{ minio_client_pass }}"
  openwebui:
    # policies managed by "openwebui" group
    secret_key: "{{ minio_client_pass }}"

minio_groups:
  admins:
    members: ["erhhung"]
    policies: ["consoleAdmin"]
  backups:
    members:
      - longhorn
      - velero
    policies: ["backups-rw"]
  metrics:
    members: ["thanos"]
    policies: ["metrics-rw"]
  openwebui:
    members: ["openwebui"]
    policies: ["openwebui-rw"]

minio_host_names: # aliases of "homelab"
  - minio # console
  - s3 # S3 API

# remember to add minio.fourteeners.local and s3.fourteeners.local to
# pfSense DNS as aliases of homelab.fourteeners.local:  192.168.0.221
# https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#products
minio_fqdns: "{{ minio_host_names | product(search_domains) | map('join','.') }}"

minio_console_fqdn: "{{ minio_fqdns[0] }}"
minio_s3api_fqdn: "{{ minio_fqdns[1] }}"

minio_operator_namespace: minio-operator
minio_tenant_namespace: minio

minio_tenant_name: homelab
minio_pool_name: qnap-pool

minio_service_host: minio.{{ minio_tenant_namespace }}.svc.{{ cluster_domain }}
minio_service_url: https://{{ minio_service_host }}
minio_public_url: https://{{ minio_s3api_fqdn }}

minio_secrets:
  root: minio-creds-root
  operator-ca: operator-ca-tls
  tenant-ca: minio-ca-tls
  console: minio-console-tls
  s3api: minio-s3-tls

# https://min.io/docs/minio/kubernetes/upstream/operations/install-deploy-manage/deploy-operator-helm.html
# https://github.com/minio/operator/tree/master/helm/operator
minio_operator_chart_version: "7.1.1"
minio_operator_release_name: operator

# https://min.io/docs/minio/kubernetes/upstream/operations/install-deploy-manage/deploy-minio-tenant-helm.html
# https://github.com/minio/operator/tree/master/helm/tenant
minio_tenant_chart_version: "7.1.1"
minio_tenant_release_name: tenant

# https://min.io/docs/minio/kubernetes/upstream/reference/operator-chart-values.html
# https://github.com/minio/operator/tree/master/helm/operator/values.yaml
minio_operator_chart_values:
  operator:
    replicaCount: 1
    # https://github.com/minio/operator/tree/master/docs/env-variables.md
    env:
      - name: SUBNET_BASE_URL
        value: ""
      - name: PROMETHEUS_NAMESPACE
        value: "{{ monitoring_namespace }}"
      - name: PROMETHEUS_NAME # kind: Prometheus
        value: "{{ monitoring_release_name }}-prometheus"

    resources:
      requests:
        cpu: 100m
        memory: 32Mi
        ephemeral-storage: 50Mi
      limits:
        cpu: 100m
        memory: 1Gi # accommodate memory surge during tenant provisioning

    # inject trusted CA certs to resolve this error in the operator pod:
    # Failed to get cluster health: certificate signed by unknown authority
    # https://github.com/minio/operator/tree/master/docs/operator-tls.md
    volumes:
      - name: operator-tls
        projected:
          sources:
            - secret:
                name: "{{ minio_secrets['operator-ca'] }}"
                items:
                  - key: public.crt
                    path: CAs/ca-0.crt
          defaultMode: 420 # "0644"
    volumeMounts:
      - name: operator-tls
        mountPath: /tmp/certs

# MinIO Operator won't create a Prometheus ServiceMonitor;
# instead, when minio_prometheus_operator is true, it will
# create minio-prom-additional-scrape-config secret in the
# Prometheus namespace and add to additionalScrapeConfigs
# of Prometheus CR (requires adding environment variables
# PROMETHEUS_NAME and PROMETHEUS_NAMESPACE to the operator
# chart values):
# https://github.com/minio/operator/tree/master/UPGRADE.md#v439---v440
# HOWEVER, the scrape config secret the operator creates
# contains hardcoded tls_config.ca_file from the service
# account, which doesn't work against our target:
# https://github.com/minio/operator/issues/940
# https://github.com/minio/operator/tree/master/pkg/resources/configmaps/prometheus.go#L99
# so we can't enable this feature yet; we instead add
# custom scrape configs to additional_scrape_configs:
minio_prometheus_operator: false

# /minio/v2/metrics/{type}
minio_collect_metrics_for:
  - cluster
  - node
  - bucket

# https://min.io/docs/minio/kubernetes/upstream/reference/tenant-chart-values.html
# https://github.com/minio/operator/tree/master/helm/tenant/values.yaml
minio_tenant_chart_values:
  tenant:
    name: "{{ minio_tenant_name }}"

    # https://min.io/docs/minio/kubernetes/upstream/reference/operator-crd.html#pool
    pools:
      - name: "{{ minio_pool_name }}"
        # servers = StatefulSet replicas (4 minimum)
        servers: "{{ groups['cluster'] | length }}"
        volumesPerServer: 1
        size: 10Gi # per volume
        storageClassName: "{{ storage_classes['nfs'] }}"

        affinity: # one pod per worker node per pool
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchExpressions:
                    - key: v1.min.io/pool
                      operator: In
                      values:
                        - "{{ minio_pool_name }}"

    features:
      # wildcard subdomains are required
      # on all these SANs in certificate
      bucketDNS: true
      domains:
        minio:
          - "{{ minio_s3api_fqdn }}"
          - minio.{{ minio_tenant_namespace }}.svc
          - minio.{{ minio_tenant_namespace }}.svc.{{ cluster_domain }}
          - "{{ minio_tenant_name }}-hl.{{ minio_tenant_namespace }}.svc.{{ cluster_domain }}"
        console: "{{ minio_console_fqdn }}"
      enableSFTP: false

    # create secret defining MINIO_ROOT_USER and MINIO_ROOT_PASSWORD environment variables:
    # https://min.io/docs/minio/linux/reference/minio-server/settings/root-credentials.html
    # https://min.io/docs/minio/kubernetes/upstream/administration/identity-access-management/minio-user-management.html#minio-root-user
    configSecret:
      name: "{{ minio_secrets['root'] }}"
      accessKey: root
      secretKey: "{{ minio_root_pass }}"

    # pre-create users (secrets containing credentials must be created manually beforehand)
    # https://min.io/docs/minio/kubernetes/upstream/reference/operator-crd.html#tenantspec
    # THIS DOES NOT WORK
    users: |
      {% set users = [] %}
      {% for name in minio_users.keys() %}
      {%   set _ = users.append({
             'name': 'minio-creds-'~ name
           }) %}
      {% endfor %}
      {{ users  }}

    # pre-create buckets
    # THIS DOES NOT WORK
    buckets: |
      {% set buckets = [] %}
      {% for name in minio_buckets.keys() %}
      {%   set _ = buckets.append({
             'name': name
           }) %}
      {% endfor  %}
      {{ buckets }}

    # https://min.io/docs/minio/kubernetes/upstream/operations/network-encryption.html
    # https://min.io/docs/minio/kubernetes/upstream/reference/operator-crd.html#localcertificatereference
    certificate:
      externalCaCertSecret:
        - name: "{{ minio_secrets['tenant-ca'] }}"
          type: Opaque
      externalCertSecret:
        - name: "{{ minio_secrets['s3api'] }}"
          type: kubernetes.io/tls
      requestAutoCert: false

    # these are default values: if true, service types will be
    # LoadBalancer, which we don't need since we use Ingresses
    # https://blog.min.io/expose-minio-eks/
    exposeServices:
      minio: false
      console: false

    metrics:
      enabled: true
      protocol: https
      port: 9000

    prometheusOperator: |
      {{ monitoring_stack_ready and
         minio_prometheus_operator }}

  ingress:
    api:
      enabled: true
      tls:
        - secretName: "{{ minio_secrets['s3api'] }}"
          hosts:
            - "{{ minio_s3api_fqdn }}"
            # enable bucket DNS feature
            - "*.{{ minio_s3api_fqdn }}"
      ingressClassName: "{{ rke_ingress_class }}"
      annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        nginx.ingress.kubernetes.io/ssl-passthrough: "true"
      # Helm template will create a
      # rule for wildcard subdomain
      host: "{{ minio_s3api_fqdn }}"

    console:
      enabled: true
      tls:
        - secretName: "{{ minio_secrets['console'] }}"
          hosts: "{{ [minio_console_fqdn] }}"
      ingressClassName: "{{ rke_ingress_class }}"
      annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        # don't use ssl-passthrough for the console because
        # a single minio container serves requests for both
        # the console and S3 API, but use the same TLS cert
        # (configured above to use minio-s3-tls secret), so
        # using passthrough will send the wrong certificate
        # to the browser!
        nginx.ingress.kubernetes.io/backend-protocol: HTTPS
      host: "{{ minio_console_fqdn }}"

# https://grafana.com/grafana/dashboards/?search=MinIO&dataSource=prometheus
minio_grafana_dashboards:
  # https://grafana.com/grafana/dashboards/13502-minio-dashboard
  - title: MinIO Dashboard
    gnetId: 13502
  # https://grafana.com/grafana/dashboards/15305-minio-replication-dashboard
  - title: MinIO Replication Dashboard
    gnetId: 15305
  # https://grafana.com/grafana/dashboards/19237-minio-bucket-dashboard
  - title: MinIO Bucket Dashboard
    gnetId: 19237
