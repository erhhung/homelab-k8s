#     monitoring_pass: {vault.yml}
#   thanos_admin_pass: {vault.yml}
#  grafana_admin_pass: {vault.yml}
#         icloud_smtp: {vault.yml}
# slack_webhook_url.*: {vault.yml}

grafana_admin_user: erhhung

# passwords will be hashed and stored
# in monitoring_passwords_hashed fact
monitoring_passwords:
  probe: "{{ monitoring_pass }}" # for Thanos probes
  metrics: "{{ monitoring_pass }}" # to scrape Thanos
  grafana: "{{ monitoring_pass }}" # to Thanos Query
  erhhung: "{{ thanos_admin_pass }}"

# send alerts or just drop
email_alerts_enabled: false
slack_alerts_enabled: true

monitoring_namespace: monitoring
# aliases of "ingress" and direct
# subdomains of fourteeners.local
monitoring_host_names:
  prometheus:
    - metrics
    - prometheus
  thanos:
    - sidecar.thanos
    - thanos # Query
  alertmanager:
    - alerts
    - alertmanager
  grafana:
    - grafana
    - monitoring

# remember to add all domain names to pfSense DNS as
# aliases of ingress.fourteeners.local: 192.168.4.222
# https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#products
monitoring_fqdns: |
  {% set fqdns = {} %}
  {% for service, hosts in monitoring_host_names.items() %}
  {%   set _ = fqdns.update({
         service: hosts | product(search_domains) | map('join','.')
       })   %}
  {% endfor %}
  {{ fqdns  }}

prometheus_service_host: "{{ monitoring_release_name }}-prometheus.{{
  monitoring_namespace }}.svc.{{ cluster_domain }}"
prometheus_service_port: 9090
prometheus_service_url: https://{{ prometheus_service_host }}:{{ prometheus_service_port }}

alertmanager_service_host: "{{ monitoring_release_name }}-alertmanager.{{
  monitoring_namespace }}.svc.{{ cluster_domain }}"
alertmanager_service_port: 9093
alertmanager_service_url: https://{{ alertmanager_service_host }}:{{ alertmanager_service_port }}

grafana_url: https://{{ monitoring_fqdns['grafana'] | first }}

monitoring_secrets:
  prometheus: monitoring-prometheus-tls
  # more thanos-* secrets to come...
  thanos-tls: thanos-sidecar-tls
  thanos-config: thanos-http-config
  alertmanager: monitoring-alertmanager-tls
  grafana: monitoring-grafana-tls
  scraper: monitoring-scraper-tls
  etcd: monitoring-etcd-tls
  oauth-proxy: monitoring-oauth-proxy
  credentials: monitoring-credentials

# https://thanos.io/tip/operating/https.md
thanos_config_secret_data:
  # in addition to the Thanos sidecar, this secret
  # will be reused many times by the bitnami/thanos
  # Helm chart, which expects this exact file name
  http-config.yml: |-
    tls_server_config:
      cert_file:       /tls/thanos/tls.crt
      key_file:        /tls/thanos/tls.key
      client_ca_file:  /tls/thanos/ca.crt
      client_auth_type: RequestClientCert
    http_server_config:
      http2: true
    basic_auth_users:
      {# https://docs.ansible.com/ansible/latest/collections/ansible/builtin/to_nice_yaml_filter.html -#}
      {# https://tedboy.github.io/jinja2/templ14.html#indent -#}
      {{ monitoring_passwords_hashed | ansible.builtin.to_nice_yaml(indent=2, sort_keys=false) |
                                                                    indent(2) }}

# `basicAuth` config for ServiceMonitors that
# scrape Thanos components (necessary because
# http-config.yml defines `basic_auth_users`)
thanos_metrics_auth:
  username:
    name: "{{ monitoring_secrets['credentials'] }}"
    key: thanos-metrics-username
  password:
    name: "{{ monitoring_secrets['credentials'] }}"
    key: thanos-metrics-password

# https://prometheus.io/docs/alerting/latest/configuration
# https://github.com/prometheus/alertmanager/tree/main/doc/examples/simple.yml
# https://prometheus-operator.dev/docs/developer/alerting#using-alertmanagerconfig-resources
# https://fabianlee.org/2022/07/03/prometheus-external-template-for-alertmanager-html-email-with-kube-prometheus-stack
alertmanager_config:
  global:
    resolve_timeout: 5m

    smtp_from: Alertmanager <{{ icloud_smtp.from_email }}>
    smtp_hello: "{{ monitoring_fqdns['alertmanager'] | first }}"
    smtp_smarthost: "{{ icloud_smtp.host }}:{{ icloud_smtp.port }}"
    smtp_require_tls: true
    smtp_auth_username: "{{ icloud_smtp.username }}"
    smtp_auth_password: "{{ icloud_smtp.passwords['alertmanager'] }}"

  # default e-mail template:
  # https://github.com/prometheus/alertmanager/tree/main/template/email.tmpl
  templates:
    - /etc/alertmanager/template/*.tmpl

  receivers:
    - name: erhhung-email
      # https://prometheus.io/docs/alerting/latest/configuration#email_config
      email_configs:
        - to: "{{ user_erhhung.email }}"
    - name: homelab-slack
      # https://prometheus.io/docs/alerting/latest/configuration#slack_config
      slack_configs:
        - channel: "#alerts"
          api_url: "{{ slack_webhook_url.alerts }}"
          send_resolved: false
    # aka /dev/null
    - name: "null"

  # https://prometheus.io/docs/alerting/latest/configuration#route
  route:
    group_by:
      - namespace
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 12h
    receiver: "null"
    routes:
      - receiver: "null"
        matchers:
          - alertname = Watchdog
      - receiver: "{{ 'erhhung-email' if email_alerts_enabled else 'null' }}"
        continue: true # send to Slack as well if enabled
      - receiver: "{{ 'homelab-slack' if slack_alerts_enabled else 'null' }}"

  # https://prometheus.io/docs/alerting/latest/configuration#inhibit_rule
  inhibit_rules:
    - source_matchers:
        - severity = critical
      target_matchers:
        - severity =~ warning|info
      equal:
        - namespace
        - alertname
    - source_matchers:
        - severity = warning
      target_matchers:
        - severity = info
      equal:
        - namespace
        - alertname
    - source_matchers:
        - alertname = InfoInhibitor
      target_matchers:
        - severity = info
      equal:
        - namespace
    - target_matchers:
        - alertname = InfoInhibitor

monitoring_oidc_client_id:
  prometheus: prometheus
  alertmanager: alertmanager

# monitoring_oidc_client_secret: {vault.yml}
# sso.fourteeners.local is same host as keycloak.fourteeners.local
monitoring_oidc_issuer_uri: https://sso.{{ homelab_domain }}/realms/homelab

# https://github.com/oauth2-proxy/oauth2-proxy/releases
monitoring_oauth2_proxy_version: "7.12.0"

# https://oauth2-proxy.github.io/oauth2-proxy/configuration/overview#general-provider-options
# https://oauth2-proxy.github.io/oauth2-proxy/configuration/providers/keycloak_oidc
# (references "redis_service_url" var from vars/valkey.yml)
monitoring_oauth2_proxy_config: |
  {% set configs = {} %}
  {% for app in [
         'prometheus',
         'alertmanager'
       ] %}
  {%   set config = {
         'provider':              'keycloak-oidc',
         'provider_display_name': 'Keycloak',
         'skip_provider_button':   true,
         'provider_ca_files':    ['/tls/server/ca.crt'],
         'client_id':        monitoring_oidc_client_id[app],
         'client_secret':    monitoring_oidc_client_secret[app],
         'code_challenge_method': 'S256',
         'redirect_url':    'https://'~ monitoring_fqdns[app][0] ~'/oauth2/callback',
         'oidc_issuer_url':  monitoring_oidc_issuer_uri,
         'cookie_secret':    oauth2_proxy_cookie_secret,
         'cookie_domains':  '.'~ search_domains[0],
         'cookie_samesite': 'lax',
         'email_domains':   ['*'],
                 'tls_cert_file': '/tls/server/tls.crt',
                 'tls_key_file':  '/tls/server/tls.key',
         'metrics_tls_cert_file': '/tls/server/tls.crt',
         'metrics_tls_key_file':  '/tls/server/tls.key',
         'session_store_type':             'redis',
         'redis_use_cluster':               true,
         'redis_insecure_skip_tls_verify':  true,
         'redis_cluster_connection_urls':  [redis_service_url],
         'redis_password':                  valkey_pass
       } %}
  {#   to_toml: filter_plugins/filters.py #}
  {%   set _ = configs.update({
         app ~'.toml': config | to_toml
       }) %}
  {% endfor  %}
  {{ configs }}

metric_scraper_tls_config:
  cert: # SecretOrConfigMap
    secret:
      name: "{{ monitoring_secrets['scraper'] }}"
      key: tls.crt
  keySecret: # SecretKeySelector
    name: "{{ monitoring_secrets['scraper'] }}"
    key: tls.key
  ca: # SecretOrConfigMap
    secret:
      name: "{{ monitoring_secrets['scraper'] }}"
      key: ca.crt
  # NOTE: Prometheus scraper cannot verify server
  # certs because it connects to targets using IP
  # addresses (from what I've read) and the certs
  # will not contain IP SANs to cover all pod IPs
  insecureSkipVerify: true

# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack
prometheus_stack_chart_version: "77.5.0"
monitoring_release_name: prometheus-stack

# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/values.yaml
# https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md
prometheus_stack_chart_values:
  # NOTE: if changing the stack/release name and re-installing the Helm release,
  # be sure to manually delete the "<stack_name>-kubelet" service in kube-system
  # namespace, or else "found duplicate series for the match group" error occurs
  nameOverride: "{{ monitoring_release_name }}"

  # https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/charts/crds
  crds:
    enabled: true
    upgradeJob: # since Helm has no ability
      enabled: true # feature is in preview

  # ======================================== Operator ========================================
  #
  # https://prometheus-operator.dev/docs
  # https://prometheus-operator.dev/docs/getting-started/installation#install-using-helm-chart
  prometheusOperator:
    enabled: true
    tls:
      enabled: true
    revisionHistoryLimit: 2

    prometheusInstanceNamespaces:
      - "{{ monitoring_namespace }}"
    thanosRulerInstanceNamespaces:
      - "{{ monitoring_namespace }}"
    alertmanagerInstanceNamespaces:
      - "{{ monitoring_namespace }}"
    alertmanagerConfigNamespaces: []

    resources:
      requests:
        cpu: 10m
        memory: 24Mi
      limits:
        cpu: 50m
        memory: 96Mi

    serviceMonitor:
      selfMonitor: true

  # ======================================= Prometheus =======================================
  #
  # https://prometheus.io/docs/prometheus/latest/configuration/configuration
  prometheus:
    enabled: true
    agentMode: false

    # view prometheus.yaml:
    # kubectl get secret -n monitoring prometheus-prometheus-stack-prometheus -o jsonpath='{.data.prometheus\.yaml\.gz}' | base64 -d | gunzip | yq
    # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#prometheusspec
    prometheusSpec:
      # total pods = replicas * shards
      replicas: 2
      shards: 1

      # soft:  prefer different nodes
      # hard: require different nodes
      podAntiAffinity: hard

      resources:
        requests:
          cpu: 250m
          memory: 1Gi

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/platform/storage.md
      storageSpec:
        volumeClaimTemplate:
          spec:
            ## no need for replicas at storage level as we
            ## already have replicas scraping same metrics
            storageClassName: "{{ storage_classes['default'] }}"
            resources:
              requests:
                storage: 8Gi

      persistentVolumeClaimRetentionPolicy:
        whenScaled: Retain
        whenDeleted: Delete

      # data retention period
      retention: 6h

      tsdb:
        # allow out-of-order samples because
        # there may be lag in a slow cluster
        outOfOrderTimeWindow: 5s

      # default intervals
      scrapeInterval: 30s
      # see related setting for Thanos Ruler:
      # thanos_chart_values.ruler.evalInterval
      evaluationInterval: 1m

      # https://prometheus-operator.dev/docs/developer/scrapeclass
      scrapeClasses: []

      # {} value selects all namespaces
      serviceMonitorNamespaceSelector: {}
      # by not providing serviceMonitorSelector and having
      # serviceMonitorSelectorNilUsesHelmValues: true, the
      # chart will use default selector that matches label:
      # release: {{ monitoring_release_name }}
      serviceMonitorSelectorNilUsesHelmValues: true
      serviceMonitorSelector: {}
      #   matchLabels:
      #     prometheus: scrape
      # exact same behavior as above
      podMonitorNamespaceSelector: {}
      podMonitorSelectorNilUsesHelmValues: true
      podMonitorSelector: {}
      ruleNamespaceSelector: {}
      ruleSelectorNilUsesHelmValues: true
      ruleSelector: {}
      probeNamespaceSelector: {}
      probeSelectorNilUsesHelmValues: true
      probeSelector: {}
      scrapeConfigNamespaceSelector: {}
      scrapeConfigSelectorNilUsesHelmValues: true
      scrapeConfigSelector: {}

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#queryspec
      query: # QuerySpec
        maxConcurrency: 4
        maxSamples: 50000
        timeout: 30s

      # https://prometheus.io/docs/prometheus/latest/feature_flags
      enableFeatures:
        # https://prometheus.io/docs/prometheus/latest/feature_flags#concurrent-evaluation-of-independent-rules
        - concurrent-rule-eval

      # allow default "pull" model only
      enableRemoteWriteReceiver: false

      externalUrl: https://{{ monitoring_fqdns['prometheus'] | first }}

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#prometheuswebspec
      web: # PrometheusWebSpec
        # WebTLSConfig (web server certificate):
        # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#webtlsconfig
        tlsConfig:
          # IMPORTANT! file-based configs using certFile/keyFile/clientCAFile
          # do NOT work--they yield an invalid pod volume SecretVolumeSource
          # for "web-config-tls-secret-key-*" that has no "secretName" field

          cert: # SecretOrConfigMap
            secret: # SecretKeySelector
              name: "{{ monitoring_secrets['prometheus'] }}"
              key: tls.crt
          # certFile: /tls/server/tls.crt
          keySecret: # SecretKeySelector
            name: "{{ monitoring_secrets['prometheus'] }}"
            key: tls.key
          # keyFile: /tls/server/tls.key
          client_ca: # SecretOrConfigMap
            secret:
              name: "{{ monitoring_secrets['prometheus'] }}"
              key: ca.crt
          # clientCAFile: /tls/server/ca.crt
          # https://pkg.go.dev/crypto/tls#ClientAuthType
          clientAuthType: RequestClientCert

      volumes:
        - name: tls-server
          secret:
            secretName: "{{ monitoring_secrets['prometheus'] }}"
        - name: tls-scraper
          secret:
            secretName: "{{ monitoring_secrets['scraper'] }}"
        - name: tls-etcd # scraper
          secret:
            secretName: "{{ monitoring_secrets['etcd'] }}"
        # provide Thanos certs for gRPC server and
        # for verifying MinIO's server certificate
        - name: tls-thanos
          secret:
            secretName: "{{ monitoring_secrets['thanos-tls'] }}"
        # provide Thanos file for --http.config
        - name: thanos-config
          secret:
            secretName: "{{ monitoring_secrets['thanos-config'] }}"
        - name: oauth-proxy
          secret:
            secretName: "{{ monitoring_secrets['oauth-proxy'] }}"

      # mounts for all containers except extras
      # like "thanos-sidecar" and "oauth-proxy"
      volumeMounts:
        - name: tls-server
          mountPath: /tls/server
          readOnly: true
        - name: tls-scraper
          mountPath: /tls/scraper
          readOnly: true
        - name: tls-etcd
          mountPath: /tls/etcd
          readOnly: true

      # inject additional containers, e.g. adding
      # an authentication proxy like oauth2-proxy
      containers:
        # perform merge patch on operator-generated "prometheus"
        - name: prometheus
          readinessProbe: &probe
            periodSeconds: 10
            timeoutSeconds: 5
          livenessProbe: *probe

        # perform merge patch on operator-generated "thanos-sidecar"
        - name: thanos-sidecar
          volumeMounts:
            - name: tls-thanos
              mountPath: /tls/thanos
              readOnly: true
            - name: thanos-config
              # /etc/thanos/config is already mounted
              # with prometheus.http-client-file.yaml
              mountPath: /etc/thanos/conf/http
              readOnly: true

        # https://github.com/oauth2-proxy/oauth2-proxy
        - name: oauth-proxy
          image: quay.io/oauth2-proxy/oauth2-proxy:v{{ monitoring_oauth2_proxy_version }}

          args:
            # https://oauth2-proxy.github.io/oauth2-proxy/configuration/overview#server-options
            # see .service.additionalPorts
            - --https-address=0.0.0.0:10090
            - --metrics-secure-address=0.0.0.0:10091
            # requires IP SAN in server cert
            - --upstream=https://127.0.0.1:9090
            # see monitoring_oauth2_proxy_config
            - --config=/etc/oauth2-proxy.cfg

          volumeMounts:
            - name: tls-server
              mountPath: /tls/server
              readOnly: true
            - name: oauth-proxy
              mountPath: /etc/oauth2-proxy.cfg
              subPath: prometheus.toml
              readOnly: true

          ports:
            - name: oauth-proxy
              containerPort: 10090
              protocol: TCP
            - name: oauth-metrics
              containerPort: 10091
              protocol: TCP

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#thanosspec
      thanos: # ThanosSpec
        objectStorageConfig:
          # create secret containing MinIO credentials:
          # https://thanos.io/tip/thanos/storage.md#s3
          secret:
            type: S3
            config:
              # both bucket and user are
              # created by vars/minio.yml
              bucket: metrics
              endpoint: "{{ minio_service_host }}"
              region: "{{ minio_region }}"
              access_key: thanos
              secret_key: "{{ minio_client_pass }}"
              http_config:
                tls_config:
                  # mounted by patched container
                  cert_file: /tls/thanos/tls.crt
                  key_file: /tls/thanos/tls.key
                  ca_file: /tls/thanos/ca.crt

        grpcServerTlsConfig:
          # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#tlsconfig
          # only certFile, keyFile, caFile are supported
          certFile: /tls/thanos/tls.crt
          keyFile: /tls/thanos/tls.key
          caFile: /tls/thanos/ca.crt

        # https://thanos.io/tip/components/sidecar.md
        additionalArgs:
          # https://thanos.io/tip/operating/https.md
          - name: http.config
            # file must match thanos_config_secret_data
            value: /etc/thanos/conf/http/http-config.yml

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#alertmanagerendpoints
      alertingEndpoints: # AlertmanagerEndpoints[]
        - name: "{{ monitoring_release_name }}-alertmanager"
          port: http-web
          scheme: https
          enableHttp2: true
          pathPrefix: /
          tlsConfig:
            certFile: /tls/server/tls.crt
            keyFile: /tls/server/tls.key
            caFile: /tls/server/ca.crt
            # must skip certificate verification here because
            # the Prometheus notifier component fails to send
            # alerts to Alertmanager due to failure to verify
            # the certificate because it does not contain any
            # IP SANs: https://10.42.xx.xx:9093/api/v2/alerts
            # and IP SAN cannot contain wildcards for pod IPs
            insecureSkipVerify: true

      # additional_scrape_configs and additional_alertmanager_configs are
      # defined at the bottom of this file. they can contain nested lists
      # that are facts set by custom tasks under tasks/monitoring/configs
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/flatten_filter.html
      additionalScrapeConfigs: "{{             additional_scrape_configs | ansible.builtin.flatten }}"
      additionalAlertManagerConfigs: "{{ additional_alertmanager_configs | ansible.builtin.flatten }}"

    # ===END=== prometheusSpec

    service:
      enabled: true
      port: 9090
      targetPort: http-web

      additionalPorts:
        - name: oauth-proxy
          port: 10090 # ingress
          targetPort: oauth-proxy
        - name: oauth-metrics
          port: 10091
          targetPort: oauth-metrics

    ingress:
      enabled: true
      tls:
        - secretName: "{{ monitoring_secrets['prometheus'] }}"
          hosts: "{{ monitoring_fqdns['prometheus'] }}"
      ingressClassName: "{{ rke_ingress_class }}"
      annotations: &ingress-annotations
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        # nginx.ingress.kubernetes.io/ssl-passthrough: "true"
        nginx.ingress.kubernetes.io/backend-protocol: HTTPS
      hosts: "{{ monitoring_fqdns['prometheus'] }}"
      paths: ["/"]
      pathType: Prefix
      servicePort: 10090 # oauth-proxy

    serviceMonitor:
      selfMonitor: true
      scheme: https

      # TLSConfig (mTLS client certificate):
      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#safetlsconfig
      tlsConfig: "{{ metric_scraper_tls_config }}"

    # for Thanos service discovery (see also
    # thanos_chart_values.query.dnsDiscovery)
    thanosService:
      enabled: true
    thanosServiceExternal:
      enabled: false
    thanosIngress:
      enabled: false

    thanosServiceMonitor:
      enabled: true
      scheme: https
      tlsConfig: "{{ metric_scraper_tls_config }}"
      # basicAuth will be patched in via task

    # for oauth-proxy sidecars
    additionalServiceMonitors: |
      {% set smons = [] %}
      {% for app in [
             'prometheus',
             'alertmanager'
           ] %}
      {%   set svc = monitoring_release_name ~'-'~ app %}
      {%   set _   = smons.append({
             'name':  svc ~'-oauth-proxy',
             'additionalLabels': {
               'app': svc
             },
             'selector': {
               'matchLabels': {
                 'app':           svc,
                 'release':       monitoring_release_name,
                 'self-monitor': 'true'
               }
             },
             'namespaceSelector': {
               'matchNames': [monitoring_namespace]
             },
             'endpoints': [{
               'port':       'oauth-metrics',
               'path':       '/metrics',
               'scheme':     'https',
               'tlsConfig':   metric_scraper_tls_config,
               'enableHttp2': true
             }]
           }) %}
      {% endfor %}
      {{ smons  }}

  # best to keep rules on Prometheus itself:
  # https://thanos.io/tip/components/rule.md
  thanosRuler:
    enabled: false

  # ====================================== Alertmanager ======================================
  #
  # https://prometheus.io/docs/alerting/alertmanager
  alertmanager:
    enabled: true

    # view alertmanager.yaml:
    # kubectl get secret -n monitoring alertmanager-prometheus-stack-alertmanager -o jsonpath='{.data.alertmanager\.yaml}' | base64 -d | yq
    # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#alertmanagerspec
    alertmanagerSpec:
      replicas: 2

      # soft:  prefer different nodes
      # hard: require different nodes
      podAntiAffinity: hard

      resources:
        requests:
          cpu: 50m
          memory: 64Mi

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/platform/storage.md
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: "{{ storage_classes['default'] }}"
            resources:
              requests:
                # min XFS volume size is 300Mi
                storage: 384Mi

      persistentVolumeClaimRetentionPolicy:
        whenScaled: Retain
        whenDeleted: Delete

      # data retention period
      retention: 72h # 3 days

      # {} value selects all namespaces
      alertmanagerConfigNamespaceSelector: {}
      #   matchLabels:
      #     prometheus: config
      # {} value selects all configs
      alertmanagerConfigSelector: {}

      externalUrl: https://{{ monitoring_fqdns['alertmanager'] | first }}
      scheme: https

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#alertmanagerwebspec
      web: # AlertmanagerWebSpec
        # WebTLSConfig (web server certificate):
        # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#webtlsconfig
        tlsConfig: &alerts_tls
          cert: # SecretOrConfigMap
            secret: # SecretKeySelector
              name: "{{ monitoring_secrets['alertmanager'] }}"
              key: tls.crt
          keySecret: # SecretKeySelector
            name: "{{ monitoring_secrets['alertmanager'] }}"
            key: tls.key
          client_ca: # SecretOrConfigMap
            secret:
              name: "{{ monitoring_secrets['alertmanager'] }}"
              key: ca.crt
          # https://pkg.go.dev/crypto/tls#ClientAuthType
          clientAuthType: RequestClientCert

      # TLSConfig (mTLS client certificate):
      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#tlsconfig
      tlsConfig: *alerts_tls

      volumes:
        - name: tls-server
          secret:
            secretName: "{{ monitoring_secrets['alertmanager'] }}"
        - name: oauth-proxy
          secret:
            secretName: "{{ monitoring_secrets['oauth-proxy'] }}"

      # inject additional containers, e.g. adding
      # an authentication proxy like oauth2-proxy
      containers:
        # perform merge patch on operator-generated "alertmanager"
        - name: alertmanager
          readinessProbe: *probe
          livenessProbe: *probe

        # https://github.com/oauth2-proxy/oauth2-proxy
        - name: oauth-proxy
          image: quay.io/oauth2-proxy/oauth2-proxy:v{{ monitoring_oauth2_proxy_version }}

          args:
            # https://oauth2-proxy.github.io/oauth2-proxy/configuration/overview#server-options
            # see .service.additionalPorts
            - --https-address=0.0.0.0:10093
            - --metrics-secure-address=0.0.0.0:10094
            # requires IP SAN in server cert
            - --upstream=https://127.0.0.1:9093
            # see monitoring_oauth2_proxy_config
            - --config=/etc/oauth2-proxy.cfg

          volumeMounts:
            - name: tls-server
              mountPath: /tls/server
              readOnly: true
            - name: oauth-proxy
              mountPath: /etc/oauth2-proxy.cfg
              subPath: alertmanager.toml
              readOnly: true

          ports:
            - name: oauth-proxy
              containerPort: 10093
              protocol: TCP
            - name: oauth-metrics
              containerPort: 10094
              protocol: TCP

    # ===END=== alertmanagerSpec

    # https://prometheus.io/docs/alerting/latest/configuration
    config: "{{ alertmanager_config }}"

    service:
      enabled: true
      port: 9093
      targetPort: http-web

      additionalPorts:
        - name: oauth-proxy
          port: 10093 # ingress
          targetPort: oauth-proxy
        - name: oauth-metrics
          port: 10094
          targetPort: oauth-metrics

    ingress:
      enabled: true
      tls:
        - secretName: "{{ monitoring_secrets['alertmanager'] }}"
          hosts: "{{ monitoring_fqdns['alertmanager'] }}"
      ingressClassName: "{{ rke_ingress_class }}"
      annotations: *ingress-annotations
      hosts: "{{ monitoring_fqdns['alertmanager'] }}"
      paths: ["/"]
      pathType: Prefix
      servicePort: 10093 # oauth-proxy

    serviceMonitor:
      selfMonitor: true
      scheme: https
      tlsConfig: "{{ metric_scraper_tls_config }}"

  # ======================================== Grafana ========================================
  #
  # https://prometheus.io/docs/visualization/grafana
  grafana: # subchart values
    enabled: true

    defaultDashboardsEnabled: true
    defaultDashboardsEditable: true
    defaultDashboardsTimezone: browser
    defaultDashboardsInterval: 5m

    # settings below are defined by Grafana Helm chart:
    # https://github.com/grafana/helm-charts/tree/main/charts/grafana
    # https://github.com/grafana/helm-charts/tree/main/charts/grafana/values.yaml
    replicas: 1
    revisionHistoryLimit: 2

    admin:
      existingSecret: "{{ monitoring_secrets['credentials'] }}"
      userKey: grafana-admin-username
      passwordKey: grafana-admin-password

    # https://docs.grafana.org/installation/configuration
    grafana.ini:
      # maps at this level correspond to INI file sections:
      # https://github.com/grafana/helm-charts/tree/main/charts/grafana/templates/_config.tpl
      server:
        # https://grafana.com/docs/grafana/latest/setup-grafana/set-up-https#configure-grafana-https-and-restart-grafana
        # https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana#server
        protocol: https
        cert_file: /tls/server/tls.crt
        cert_key: /tls/server/tls.key
      # https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana#security
      security:
        admin_user: "{{ user_erhhung.username }}"
        admin_email: "{{ user_erhhung.email }}"
        # there doesn't appear to be CORS settings available,
        # so CORS headers are added by ingress server snippet
        strict_transport_security: true
        allow_embedding: true

    readinessProbe:
      httpGet:
        scheme: HTTPS
    livenessProbe:
      httpGet:
        scheme: HTTPS

    persistence:
      enabled: true
      type: StatefulSet
      storageClassName: "{{ storage_classes['default'] }}"
      accessModes: ["ReadWriteOnce"]
      size: 2Gi
      # allow PVC deletion when
      # Helm release is deleted
      finalizers: []

    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 1Gi

    # https://github.com/kiwigrid/k8s-sidecar
    sidecar:
      dashboards:
        enabled: true
        env: &sidecar_env
          REQ_SKIP_TLS_VERIFY: "true"
          PYTHONWARNINGS: ignore:Unverified HTTPS request
        reloadURL: https://localhost:3000/api/admin/provisioning/dashboards/reload
        enableNewTablePanelSyntax: true

      datasources:
        enabled: true
        env: *sidecar_env
        reloadURL: https://localhost:3000/api/admin/provisioning/datasources/reload
        # disable default (Prometheus) data source because its endpoint URL in
        # the Helm template does not use HTTPS, and there is no way to specify
        # jsonData.tlsAuthWithCACert and secureJsonData.tlsCACert using values
        # in this block. Instead, we use the additionalDataSources block below
        defaultDatasourceEnabled: false
        alertmanager:
          # due to the same issue as above, we
          # define using additionalDataSources
          enabled: false

      plugins:
        enabled: false
      alerts:
        enabled: false
      notifiers:
        enabled: false

    # https://grafana.com/docs/grafana/latest/administration/provisioning#datasources
    # https://boredconsultant.com/2022/06/27/Grafana-Datasource-With-Custom-CA-Certificate
    additionalDataSources:
      - name: Prometheus
        type: prometheus
        uid: prometheus
        # url: https://{{ monitoring_release_name }}-prometheus.{{
        #   monitoring_namespace }}.svc.{{ cluster_domain }}:9090
        # Thanos Query Frontend requires downstream Tripper config with tls_config settings!
        url: https://thanos-query-frontend.{{ monitoring_namespace }}.svc.{{ cluster_domain }}:9090
        access: proxy
        # apparently Grafana ignores basic auth settings if data source access is proxy:
        # https://github.com/grafana/grafana/tree/main/pkg/api/frontendsettings.go#L517
        basicAuth: true
        basicAuthUser: grafana
        withCredentials: true
        jsonData:
          timeInterval: 30s
          prometheusType: Thanos
          tlsAuthWithCACert: true
          # need to provide basic auth credentials using
          # Authorization header (.secureJsonData value):
          # https://grafana.com/docs/grafana/latest/administration/provisioning#custom-http-headers-for-data-sources
          httpHeaderName1: Authorization
        secureJsonData:
          tlsCACert: "{{ ca_certificates | join('\n') }}"
          basicAuthPassword: "{{ monitoring_passwords['grafana'] }}"
          httpHeaderValue1: Basic {{ ('grafana:'~ monitoring_passwords['grafana']) | b64encode }}
        isDefault: true

      - name: Alertmanager
        type: alertmanager
        uid: alertmanager
        url: https://{{ monitoring_release_name }}-alertmanager.{{
          monitoring_namespace }}.svc.{{ cluster_domain }}:9093
        access: proxy
        jsonData:
          tlsAuthWithCACert: true
          implementation: prometheus
          handleGrafanaManagedAlerts: false
        secureJsonData:
          tlsCACert: "{{ ca_certificates | join('\n') }}"

    extraSecretMounts:
      - name: tls-server
        secretName: "{{ monitoring_secrets['grafana'] }}"
        mountPath: /tls/server
        readOnly: true

    # inject additional containers, e.g. adding
    # an authentication proxy like oauth2-proxy
    extraContainers: ""

    service:
      port: 443
      targetPort: 3000

    ingress:
      enabled: true
      tls:
        - secretName: "{{ monitoring_secrets['grafana'] }}"
          hosts: "{{ monitoring_fqdns['grafana'] }}"
      ingressClassName: "{{ rke_ingress_class }}"
      annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        # cannot use ssl-passthrough with server-snippet
        # because ssl-passthrough works at OSI layer 4,
        # and it will invalidate all other annotations
        nginx.ingress.kubernetes.io/backend-protocol: HTTPS
        # add CORS headers on /api to allow sites like
        # https://editor-next.swagger.io/ invoke APIs
        nginx.ingress.kubernetes.io/server-snippet: |
          {{ grafana_ingress_snippet }}
        #
        # if ($host !~* ^({{ monitoring_host_names['grafana'] | join('|') }})\.) {
        #   # send Misdirected Request response
        #   # (result of connection coalescing)
        #   return 421;
        # }
      hosts: "{{ monitoring_fqdns['grafana'] }}"
      path: /
      pathType: Prefix

    serviceMonitor:
      enabled: true
      labels:
        # Prometheus Operator will not be able to find
        # this ServiceMonitor without these labels and
        # the Grafana Overview dashboard will be empty:
        # https://github.com/prometheus-community/helm-charts/issues/3800
        app: "{{ monitoring_release_name }}-grafana"
        release: "{{ monitoring_release_name }}"
      scheme: https
      tlsConfig: "{{ metric_scraper_tls_config }}"

  # =================================== kube-state-metrics ===================================
  #
  # https://github.com/kubernetes/kube-state-metrics
  kubeStateMetrics:
    enabled: true

  # https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics
  # https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics/values.yaml
  kube-state-metrics: # subchart values
    resources:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 50m
        memory: 128Mi

    volumes:
      # volume to be mounted in
      # kube-rbac-proxy sidecar
      - name: tls-proxy
        secret:
          secretName: "{{ monitoring_secrets['prometheus'] }}"

    # https://github.com/brancz/kube-rbac-proxy#usage
    kubeRBACProxy:
      # disabling for now due to bug in the Helm chart
      # with readiness and liveness probes using HTTPS:
      # https://github.com/prometheus-community/helm-charts/issues/5529
      enabled: false

      volumeMounts:
        - name: tls-proxy
          mountPath: /tls/proxy
          readOnly: true

      extraArgs:
        - --tls-cert-file=/tls/proxy/tls.crt
        - --tls-private-key-file=/tls/proxy/tls.key
        - --client-ca-file=/tls/proxy/ca.crt

    prometheus:
      monitor:
        enabled: true
        # disable HTTPS due to kubeRBACProxy.
        # values below apply to same settings
        # under .http and .metrics endpoints
        # scheme: https
        # tlsConfig: "{{ metric_scraper_tls_config }}"
        # enableHttp2: true

    collectors:
      - certificatesigningrequests
      - configmaps
      - cronjobs
      - daemonsets
      - deployments
      - endpoints
      - horizontalpodautoscalers
      - ingresses
      - jobs
      - leases
      - limitranges
      - mutatingwebhookconfigurations
      - namespaces
      - networkpolicies
      - nodes
      - persistentvolumeclaims
      - persistentvolumes
      - poddisruptionbudgets
      - pods
      - replicasets
      - replicationcontrollers
      - resourcequotas
      - secrets
      - services
      - statefulsets
      - storageclasses
      - validatingwebhookconfigurations
      - volumeattachments
      # - ingressclasses
      # - clusterrolebindings
      # - clusterroles
      # - roles

  # ====================================== Node Exporter =====================================
  #
  # https://github.com/prometheus/node_exporter
  nodeExporter: # deploy DaemonSet
    enabled: true

    operatingSystems:
      linux:
        enabled: true
      aix:
        enabled: false
      darwin:
        enabled: false

  # https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter
  # https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter/values.yaml
  prometheus-node-exporter: # subchart values
    resources:
      requests:
        cpu: 10m
        memory: 128Mi
      limits:
        cpu: 50m
        memory: 256Mi

    extraVolumes:
      - name: tls-proxy
        secret:
          secretName: "{{ monitoring_secrets['prometheus'] }}"

    # https://github.com/brancz/kube-rbac-proxy#usage
    kubeRBACProxy:
      enabled: true

      extraVolumeMounts:
        - name: tls-proxy
          mountPath: /tls/proxy
          readOnly: true

      extraArgs:
        # default args already specify --config-file, but mounted
        # ConfigMap will be overridden by ".extraManifests" below
        - --tls-cert-file=/tls/proxy/tls.crt
        - --tls-private-key-file=/tls/proxy/tls.key
        - --client-ca-file=/tls/proxy/ca.crt

    prometheus:
      monitor:
        enabled: true
        scheme: https
        tlsConfig: "{{ metric_scraper_tls_config }}"
        scrapeTimeout: 30s

        # https://training.promlabs.com/training/relabeling/introduction-to-relabeling/hidden-labels-and-metadata
        relabelings:
          # https://prometheus.io/docs/prometheus/latest/configuration/configuration#kubernetes_sd_config
          - sourceLabels: ["__meta_kubernetes_pod_node_name"]
            targetLabel: node

  # ======================================== Exporters =======================================
  #
  kubelet:
    enabled: true

  # requires RKE2 server options "etcd-expose-metrics: true"
  # and "etcd-arg: - listen-metrics-urls=http://node-ip:2381"
  kubeEtcd:
    enabled: true
    service:
      enabled: true
      selector:
        component: etcd

    serviceMonitor:
      enabled: true
      scheme: http
      # must use file-based certs
      # certFile: /tls/etcd/tls.crt
      # keyFile: /tls/etcd/tls.key
      # caFile: /tls/etcd/ca.crt

  kubeApiServer:
    enabled: true

  # requires component arg
  # "bind-address=0.0.0.0"
  kubeControllerManager:
    enabled: true
    service:
      enabled: true
      selector:
        component: kube-controller-manager

  # requires component arg
  # "bind-address=0.0.0.0"
  kubeScheduler:
    enabled: true
    service:
      enabled: true
      selector:
        component: kube-scheduler

  kubeProxy:
    enabled: true
    service:
      enabled: true
      selector:
        component: kube-proxy

  coreDns: # no kubeDns
    enabled: true
    service:
      enabled: true
      selector:
        k8s-app: kube-dns

  # ======================================== Manifests =======================================
  #
  extraManifests:
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        # this overrides the ConfigMap created by the prometheus-node-exporter
        # Helm chart because the provided resource-based authorization doesn't
        # work--maybe the ServiceAccount doesn't have ClusterRole that allows
        # "tokenreviews" & "subjectaccessreviews"? So we use static rule that
        # only allows the client cert holder to access the /metrics endpoint:
        # https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter/templates/rbac-configmap.yaml
        # https://github.com/brancz/kube-rbac-proxy/tree/master/examples/static-auth
        name: "{{ monitoring_release_name }}-prometheus-node-exporter-rbac-config"
      data:
        config-file.yaml: |+
          authorization:
            static:
              - user: # cert CN
                  name: scraper
                verb: get
                resourceRequest: false
                path: /metrics

  # additional_prometheus_rules is defined at bottom of this file
  additionalPrometheusRulesMap: "{{ additional_prometheus_rules }}"

# ===END=== prometheus_stack_chart_values

# PUT https://grafana.fourteeners.local/api
grafana_admin_settings:
  # /user
  profile:
    login: "{{ grafana_admin_user }}"
    name: "{{ user_erhhung.fullname }}"
    email: "{{ user_erhhung.email }}"
  # /user/preferences
  preferences:
    theme: dark
    language: en-US
    timezone: browser
    weekStart: monday
  # /user/helpflags/{n}
  help_flags: [1]

# ========================================== Additions ======================================
# ===== additional configurations supported by kube-prometheus-stack chart values, such =====
# ===== as scrape and Alertmanager configurations, for components not part of the stack =====
# ===========================================================================================

# additional_scrape_configs allows supplying additional Prometheus scrape configurations.
# Scrape configurations are appended to the configurations generated by the Prometheus
# Operator. Job configurations must have form specified in the Prometheus documentation:
# https://prometheus.io/docs/prometheus/latest/configuration/configuration#scrape_config
# additional_scrape_configs can be defined as a list or as a templated string.
#
# As scrape configs are appended, the user is responsible to ensure they're valid.
# Note that using this feature may break Prometheus upgrades, so it's advised to
# review Prometheus release notes to ensure that no incompatible scrape configs
# are going to break Prometheus after an upgrade.
#
# The scrape configuration example below will find master nodes provided their name
# matches .*mst.*, relabel the port to 2379, and allow etcd scraping provided it's
# running on all Kubernetes master nodes.
#
# additional_scrape_configs:
#   - job_name: kube-etcd
#     kubernetes_sd_configs:
#       - role: node
#     scheme: https
#     tls_config:
#       ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
#       cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
#       key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
#     relabel_configs:
#       - action: labelmap
#         regex: __meta_kubernetes_node_label_(.+)
#       - source_labels: [__address__]
#         action: replace
#         targetLabel: __address__
#         regex: ([^:;]+):(\d+)
#         replacement: ${1}:2379
#       - source_labels: [__meta_kubernetes_node_name]
#         action: keep
#         regex: .*mst.*
#       - source_labels: [__meta_kubernetes_node_name]
#         action: replace
#         targetLabel: node
#         regex: (.*)
#         replacement: ${1}
#     metric_relabel_configs:
#       - regex: (kubernetes_io_hostname
#                |beta_kubernetes_io_os
#                |beta_kubernetes_io_arch
#                |beta_kubernetes_io_instance_type
#                |failure_domain_beta_kubernetes_io_region
#                |failure_domain_beta_kubernetes_io_zone)
#         action: labeldrop
#
additional_scrape_configs:
  - "{{ static_scrape_configs | default([]) }}" # tasks/monitoring/configs/static.yml
  - "{{  minio_scrape_configs | default([]) }}" # tasks/monitoring/configs/minio.yml

# additional_alertmanager_configs allows supplying additional Alertmanager job configurations.
# Alertmanager configurations are appended to the configurations generated by the Prometheus
# Operator. Job configurations must have the form specified in the Prometheus documentation:
# https://prometheus.io/docs/prometheus/latest/configuration/configuration#alertmanager_config
#
# As Alertmanager configs are appended, the user is responsible to ensure they're valid.
# Note that using this feature may break Prometheus upgrades, so it's advised to review
# Prometheus release notes to ensure that no incompatible Alertmanager configs are going
# to break Prometheus after an upgrade.
#
# additional_alertmanager_configs:
#   - consul_sd_configs:
#       - server: consul.dev.test:8500
#         scheme: http
#         datacenter: dev
#         tag_separator: ','
#         services:
#           - metrics-prometheus-alertmanager
#
additional_alertmanager_configs: []

# custom recording/alerting rules
#
# additional_prometheus_rules:
#   rule-name:
#     groups:
#       - name: my_group
#         rules:
#           - record: my_record
#             expr: 100 * my_record
#
additional_prometheus_rules: {}
