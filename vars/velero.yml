# velero_repo_pass:  {vault.yml}
# velero_admin_pass: {vault.yml}
# velero_passphrase: {vault.yml}

velero_namespace: velero
velero_ui_namespace: velero
velero_host_name: velero # alias of "ingress"

# remember to add velero.fourteeners.local to pfSense DNS
# as an alias of ingress.fourteeners.local: 192.168.4.222
# https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#products
velero_fqdn: "{{ [velero_host_name] | product(search_domains) | map('join','.') | first }}"

velero_secrets:
  minio: minio-credentials
  creds: velero-ui-credentials
  ingress: velero-ingress-tls

# https://github.com/vmware-tanzu/helm-charts/tree/main/charts/velero
velero_chart_version: "10.0.12"
velero_release_name: velero

# https://github.com/otwld/helm-charts/tree/main/charts/velero-ui
velero_ui_chart_version: "0.13.3"
velero_ui_release_name: velero-ui

# https://github.com/vmware-tanzu/helm-charts/tree/main/charts/velero/values.yaml
velero_chart_values:
  upgradeCRDs: true
  cleanUpCRDs: true

  # this is how storage provider plugins get
  # installed (at least one must be defined)
  # NOTE: velero-plugin-for-csi is now part
  # of Velero (as of v1.14), so must not be
  # included here
  initContainers:
    # use the AWS plugin to connect to MinIO:
    # https://github.com/vmware-tanzu/velero-plugin-for-aws
    - name: aws-plugin
      image: docker.io/velero/velero-plugin-for-aws
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - name: plugins
          mountPath: /target

  podSecurityContext: &pod_security
    fsGroup: 2000
  containerSecurityContext: &container_security
    capabilities:
      drop: ["ALL"]
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  resources:
    requests:
      cpu: 50m
      memory: 128Mi

  # define default backup location--
  # create BackupStorageLocation CRD
  backupsEnabled: true
  # enable volume snapshots feature--
  # create VolumeSnapshotLocation CRD
  snapshotsEnabled: true
  # disable Restic file-level backups
  deployNodeAgent: false

  # create Secret with MinIO credentials
  # referenced by BackupStorageLocation
  credentials:
    useSecret: true
    name: "{{ velero_secrets['minio'] }}"
    secretContents:
      cloud: |
        [default]
        aws_access_key_id = velero
        aws_secret_access_key = {{ minio_client_pass }}

  configuration:
    # server-level settings passed as CLI args
    # (run `velero server --help` for details)
    # https://velero.io/docs/latest/csi
    features: EnableCSI
    logLevel: info # default=info
    uploaderType: kopia # or restic
    backupSyncPeriod: 5m # default=1m
    storeValidationFrequency: 10m # default=1m

    # "default" BackupStorageLocation:
    # https://velero.io/docs/latest/api-types/backupstoragelocation
    # https://github.com/vmware-tanzu/velero-plugin-for-aws/tree/main/backupstoragelocation.md
    backupStorageLocation:
      - name: default
        default: true
        provider: aws
        bucket: backups
        prefix: velero
        caCert: "{{ ca_certificates[1] | b64encode }}"
        credential:
          name: "{{ velero_secrets['minio'] }}"
          key: cloud
        config:
          region: "{{ minio_region }}"
          s3Url: "{{ minio_service_url }}"
          publicUrl: "{{ minio_public_url }}"
          s3ForcePathStyle: "true"

    # don't create VolumeSnapshotLocation because the "aws" provider
    # is for taking EBS snapshots, not Longhorn, CSI-based snapshots
    # https://velero.io/docs/latest/api-types/volumesnapshotlocation
    volumeSnapshotLocation: []

  metrics:
    enabled: true
    scrapeInterval: 30s
    scrapeTimeout: 10s

    serviceMonitor: &prometheus
      enabled: "{{ prometheus_crds_installed }}"
      additionalLabels:
        release: "{{ monitoring_release_name }}"

    prometheusRule:
      <<: *prometheus
      spec:
        - alert: VeleroBackupFailures
          labels:
            severity: warning
          annotations:
            message: >-
              {% raw -%}
              Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} failed backups.
              {%- endraw %}
          expr: >-
            velero_backup_failure_total{schedule!=""} / velero_backup_attempt_total{schedule!=""} > 0.25
          for: 15m

  extraObjects:
    # this secret is created automatically by Velero (not part of Helm release)
    # and uses the same "static-passw0rd", so let's explicitly set our own repo
    # password for Kopia/Restic repository encryption (and add to Helm release)
    - apiVersion: v1
      kind: Secret
      metadata:
        name: velero-repo-credentials # expected!
      type: Opaque
      data:
        repository-password: "{{ velero_repo_pass | b64encode }}"

    # create schedule to backup entire cluster daily:
    # https://velero.io/docs/main/api-types/schedule
    - apiVersion: velero.io/v1
      kind: Schedule
      metadata:
        name: cluster-daily
      spec:
        # use cron expression because "@every 24h" cannot indicate start hour
        schedule: 0 12 * * * # in UTC time; run every day at 4am Pacific time
        useOwnerReferencesInBackup: false
        template:
          metadata:
            labels:
              velero.io/schedule-name: cluster-daily
          excludedNamespaces:
            - vcluster-*
          itemOperationTimeout: 10m
          csiSnapshotTimeout: 30m
          ttl: 168h0m0s # 1 week

# https://github.com/otwld/helm-charts/tree/main/charts/velero-ui/values.yaml
velero_ui_chart_values:
  replicaCount: 1

  configuration:
    general:
      veleroNamespace: "{{ velero_namespace }}"
      # "EbXSjT24k" is the installed uid of Velero Dashboard 16829
      grafanaUrl: "{{ grafana_url }}/d/EbXSjT24k/velero-dashboard"

      # passphrase used to sign JWT tokens
      secretPassPhrase:
        useSecret: true
        existingSecret: "{{ velero_secrets['creds'] }}"
        key: jwt-passphrase

    # RBAC feature requires federated authentication:
    # https://velero-ui.docs.otwld.com/sso-ldap/rbac
    policies:
      enabled: false

  # https://velero-ui.docs.otwld.com/getting-started/environment-variables
  env:
    - name: DEFAULT_LANGUAGE
      value: en
    - name: DEFAULT_TIMEZONE
      value: America/Los_Angeles
    - name: NODE_ENV
      value: production
    - name: BASIC_AUTH_USERNAME
      value: admin
    - name: BASIC_AUTH_PASSWORD
      valueFrom:
        secretKeyRef:
          name: "{{ velero_secrets['creds'] }}"
          key: admin-password
    - name: AUTH_SESSION_DURATION
      value: 8h
    # https://velero-ui.docs.otwld.com/certificate
    - name: NODE_EXTRA_CA_CERTS
      value: /certs/ca.crt

  podSecurityContext: *pod_security
  securityContext: *container_security

  resources:
    requests:
      cpu: 50m
      memory: 128Mi

  readinessProbe: &probe
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 6
  livenessProbe: *probe

  volumes:
    - name: certs
      secret:
        secretName: "{{ velero_secrets['ingress'] }}"

  volumeMounts:
    - name: certs
      # environment variable
      # NODE_EXTRA_CA_CERTS
      mountPath: /certs/ca.crt
      subPath: ca.crt
      readOnly: true

  ingress:
    enabled: true
    tls:
      - secretName: "{{ velero_secrets['ingress'] }}"
        hosts: "{{ [velero_fqdn] }}"
    className: "{{ rke_ingress_class }}"
    annotations:
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
    hosts:
      - host: "{{ velero_fqdn }}"
        paths:
          - path: /
            pathType: Prefix

# https://grafana.com/grafana/dashboards/?search=Velero&dataSource=prometheus
velero_grafana_dashboards:
  # https://grafana.com/grafana/dashboards/16829-kubernetes-tanzu-velero
  - title: Velero Dashboard
    gnet_id: 16829
    tags:
      - storage
