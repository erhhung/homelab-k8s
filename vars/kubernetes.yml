rke_ha_mode: true # deploy 3 control plane nodes with kube-vip
rke_control_plane_group: "{{ 'control_plane_ha' if rke_ha_mode else 'control_plane' }}"
rke_workers_group: "{{ 'workers_ha' if rke_ha_mode else 'workers' }}"

# control plane hosts
k3s_control_plane_host: rancher
rke_control_plane_host: "{{ groups[rke_control_plane_group] | first }}"

cluster_domain: cluster.local

# K3s and RKE clusters
additional_node_labels:
  # all cluster nodes, including control plane node(s),
  # can run user workloads, so they are labeled worker
  # role (control plane nodes have already been labeled
  # with roles: "control-plane", "etcd", and "master")
  - label: node-role.kubernetes.io/worker
    value: "true"
    nodes: "{{ groups['cluster'] }}"

  # k8s6 has the most memory and ephemeral storage
  - label: node-role.kubernetes.io/runner
    value: "true"
    nodes: ["k8s6"]

  # k8s5 is running on XCP-ng host with
  # Intel Alder Lake-P GT1 UHD Graphics
  # k8s6 is running on XCP-ng host with
  # Intel Raptor Lake-P Iris Xe Graphics
  - label: feature.node.kubernetes.io/gpu.preferred
    value: "true"
    nodes:
      - k8s5
      - k8s6

# location exported by KUBECONFIG variable
k3s_kubeconfig: /etc/rancher/k3s/k3s.yaml
rke_kubeconfig: /etc/rancher/rke2/rke2.yaml

# location exported by CRI_CONFIG_FILE variable
k3s_crictl_config: /var/lib/rancher/k3s/agent/etc/crictl.yaml
rke_crictl_config: /var/lib/rancher/rke2/agent/etc/crictl.yaml

rke_tls_dir: /var/lib/rancher/rke2/server/tls
rke_bin_dir: /var/lib/rancher/rke2/bin

container_runtime: containerd
containerd_config: /var/lib/rancher/rke2/agent/etc/containerd/config.toml
# location exported by CONTAINER_RUNTIME_ENDPOINT variable
containerd_socket: /run/k3s/containerd/containerd.sock

# https://docs.k3s.io/installation/packaged-components
k3s_manifests_dir: /var/lib/rancher/k3s/server/manifests
# https://docs.rke2.io/advanced#auto-deploying-manifests
rke_manifests_dir: /var/lib/rancher/rke2/server/manifests

k3s_cluster_name: rancher
rke_cluster_name: homelab
rke_cluster_desc: Erhhung's Kubernetes Cluster at Home

# remember to add rancher.fourteeners.local to pfSense DNS!
# https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#products
k3s_fqdn: "{{ [k3s_cluster_name] | product(search_domains) | map('join','.') | first }}"

# default CNs: kubernetes, kubernetes.default, kubernetes.default.svc,
# kubernetes.default.svc.cluster.local, cluster.local, localhost, k8s?
rke_fqdns: >
  {{ ([rke_cluster_name, 'rke'] + groups[rke_control_plane_group]) |
      product(search_domains) | map('join','.') }}

kube_system_domain: kube-system.svc.{{ cluster_domain }}
k3s_dns_host: kube-dns.{{ kube_system_domain }}
rke_dns_host: rke2-coredns-rke2-coredns.{{ kube_system_domain }}

k3s_ingress_class: traefik
rke_ingress_class: nginx
# for SSL passthrough connections only
rke_ingress_pt_class: nginx-passthrough

# remember to create ingress.pem with SANs:
#    ingress.fourteeners.local
#  ingresspt.fourteeners.local
#      nginx.fourteeners.local
#    nginxpt.fourteeners.local
#  192.168.4.222..223
#  192.168.0.171..178
nginx_tls_secret: rke2-ingress-nginx-default-tls

nginx_tcp_services:
  # https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services
  2222: gitlab/gitlab-gitlab-shell:2222
nginx_udp_services: {}

# ensure names match those defined
# in manifests/priorityclasses.yaml
priority_classes:
  # "critical" is created automatically by RKE2
  critical: system-cluster-critical # 2000000000
  database: database-priority # 10000

# maximum number of revisions saved per release:
# https://docs.ansible.com/ansible/latest/collections/kubernetes/core/helm_module.html
helm_max_history: 3
