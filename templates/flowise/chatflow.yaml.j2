# this chatflow was exported from the UI and
# then templated using non-standard "[%" and
# "%]" variable markers because Flowise uses
# "{{ ... }}" to reference other graph nodes
#
# Ansible var substitutions:
#   ollama_service_url
#   ollama_model
#   doc_store_id
---
nodes:
  - id: conversationalRetrievalQAChain_0
    position:
      x: 2675
      y: 475
    type: customNode
    data:
      id: conversationalRetrievalQAChain_0
      label: Conversational Retrieval QA Chain
      version: 3
      name: conversationalRetrievalQAChain
      type: ConversationalRetrievalQAChain
      baseClasses:
        - ConversationalRetrievalQAChain
        - BaseChain
        - Runnable
      category: Chains
      description: Document QA - built on RetrievalQAChain to provide a chat history component
      inputParams:
        - label: Return Source Documents
          name: returnSourceDocuments
          type: boolean
          optional: true
          id: conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean
          display: true
        - label: Rephrase Prompt
          name: rephrasePrompt
          type: string
          description: Using previous chat history, rephrase question into a standalone question
          warning: 'Prompt must include input variables: {chat_history} and {question}'
          rows: 4
          additionalParams: true
          optional: true
          default: |-
            Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.

            Chat History:
            {chat_history}
            Follow Up Input: {question}
            Standalone Question:
          id: conversationalRetrievalQAChain_0-input-rephrasePrompt-string
          display: true
        - label: Response Prompt
          name: responsePrompt
          type: string
          description: Taking the rephrased question, search for answer from the provided context
          warning: 'Prompt must include input variable: {context}'
          rows: 4
          additionalParams: true
          optional: true
          default: |-
            I want you to act as a document that I am having a conversation with. Your name is "AI Assistant".
            Using the provided context, answer the user's question to the best of your ability using the resources provided.
            If there is nothing in the context relevant to the question at hand, just say "Hmm, I'm not sure" and stop after that.
            Refuse to answer any question not about the info. Never break character.
            ------------
            {context}
            ------------
            REMEMBER: If there is no relevant information within the context, just say "Hmm, I'm not sure".
            Don't try to make up an answer. Never break character.
          id: conversationalRetrievalQAChain_0-input-responsePrompt-string
          display: true
      inputAnchors:
        - label: Chat Model
          name: model
          type: BaseChatModel
          id: conversationalRetrievalQAChain_0-input-model-BaseChatModel
          display: true
        - label: Vector Store Retriever
          name: vectorStoreRetriever
          type: BaseRetriever
          id: conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever
          display: true
        - label: Memory
          name: memory
          type: BaseMemory
          optional: true
          description: If left empty, a default BufferMemory will be used
          id: conversationalRetrievalQAChain_0-input-memory-BaseMemory
          display: true
        - label: Input Moderation
          description: Detect text that could generate harmful output and prevent it from being sent to the language model
          name: inputModeration
          type: Moderation
          optional: true
          list: true
          id: conversationalRetrievalQAChain_0-input-inputModeration-Moderation
          display: true
      inputs:
        model: '{{chatOllama_0.data.instance}}'
        vectorStoreRetriever: '{{documentStoreVS_0.data.instance}}'
        memory: '{{bufferMemory_0.data.instance}}'
        returnSourceDocuments: false
        rephrasePrompt: |-
          Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.

          Chat History:
          {chat_history}
          Follow Up Input: {question}
          Standalone Question:
        responsePrompt: |-
          I want you to act as a document that I am having a conversation with. Your name is "AI Assistant".
          Using the provided context, answer the user's question to the best of your ability using the resources provided.
          If there is nothing in the context relevant to the question at hand, just say "Hmm, I'm not sure" and stop after that.
          Refuse to answer any question not about the info. Never break character.
          ------------
          {context}
          ------------
          REMEMBER: If there is no relevant information within the context, just say "Hmm, I'm not sure".
          Don't try to make up an answer. Never break character.
        inputModeration: ""
      outputAnchors:
        - id: conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable
          name: conversationalRetrievalQAChain
          label: ConversationalRetrievalQAChain
          description: Document QA - built on RetrievalQAChain to provide a chat history component
          type: ConversationalRetrievalQAChain | BaseChain | Runnable
      outputs: {}
      selected: false
    width: 300
    height: 581
    selected: false
    positionAbsolute:
      x: 2675
      y: 475
    dragging: false
  - id: chatOllama_0
    position:
      x: 2300
      y: 400
    type: customNode
    data:
      id: chatOllama_0
      label: ChatOllama
      version: 5
      name: chatOllama
      type: ChatOllama
      baseClasses:
        - ChatOllama
        - ChatOllama
        - BaseChatModel
        - BaseLanguageModel
        - Runnable
      category: Chat Models
      description: Chat completion using open-source LLM on Ollama
      inputParams:
        - label: Base URL
          name: baseUrl
          type: string
          default: http://localhost:11434
          id: chatOllama_0-input-baseUrl-string
          display: true
        - label: Model Name
          name: modelName
          type: string
          placeholder: llama2
          id: chatOllama_0-input-modelName-string
          display: true
        - label: Temperature
          name: temperature
          type: number
          description: 'The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8). Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details'
          step: 0.1
          default: 0.9
          optional: true
          id: chatOllama_0-input-temperature-number
          display: true
        - label: Allow Image Uploads
          name: allowImageUploads
          type: boolean
          description: Allow image input. Refer to the <a href="https://docs.flowiseai.com/using-flowise/uploads#image" target="_blank">docs</a> for more details.
          default: false
          optional: true
          id: chatOllama_0-input-allowImageUploads-boolean
          display: true
        - label: Streaming
          name: streaming
          type: boolean
          default: true
          optional: true
          additionalParams: true
          id: chatOllama_0-input-streaming-boolean
          display: true
        - label: JSON Mode
          name: jsonMode
          type: boolean
          description: 'Coerces model outputs to only return JSON. Specify in the system prompt to return JSON. Ex: Format all responses as JSON object'
          optional: true
          additionalParams: true
          id: chatOllama_0-input-jsonMode-boolean
          display: true
        - label: Keep Alive
          name: keepAlive
          type: string
          description: How long to keep connection alive. A duration string (such as "10m" or "24h")
          default: 5m
          optional: true
          additionalParams: true
          id: chatOllama_0-input-keepAlive-string
          display: true
        - label: Top P
          name: topP
          type: number
          description: 'Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9). Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details'
          step: 0.1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-topP-number
          display: true
        - label: Top K
          name: topK
          type: number
          description: 'Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40). Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details'
          step: 1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-topK-number
          display: true
        - label: Mirostat
          name: mirostat
          type: number
          description: 'Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details'
          step: 1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-mirostat-number
          display: true
        - label: Mirostat ETA
          name: mirostatEta
          type: number
          description: 'Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details'
          step: 0.1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-mirostatEta-number
          display: true
        - label: Mirostat TAU
          name: mirostatTau
          type: number
          description: 'Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details'
          step: 0.1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-mirostatTau-number
          display: true
        - label: Context Window Size
          name: numCtx
          type: number
          description: 'Sets the size of the context window used to generate the next token. (Default: 2048) Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details'
          step: 1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-numCtx-number
          display: true
        - label: Number of GPU
          name: numGpu
          type: number
          description: The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details
          step: 1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-numGpu-number
          display: true
        - label: Number of Thread
          name: numThread
          type: number
          description: Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details
          step: 1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-numThread-number
          display: true
        - label: Repeat Last N
          name: repeatLastN
          type: number
          description: 'Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx). Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details'
          step: 1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-repeatLastN-number
          display: true
        - label: Repeat Penalty
          name: repeatPenalty
          type: number
          description: 'Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1). Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details'
          step: 0.1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-repeatPenalty-number
          display: true
        - label: Stop Sequence
          name: stop
          type: string
          rows: 4
          placeholder: 'AI assistant:'
          description: Sets the stop sequences to use. Use comma to seperate different sequences. Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details
          optional: true
          additionalParams: true
          id: chatOllama_0-input-stop-string
          display: true
        - label: Tail Free Sampling
          name: tfsZ
          type: number
          description: 'Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (Default: 1). Refer to <a target="_blank" href="https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">docs</a> for more details'
          step: 0.1
          optional: true
          additionalParams: true
          id: chatOllama_0-input-tfsZ-number
          display: true
      inputAnchors:
        - label: Cache
          name: cache
          type: BaseCache
          optional: true
          id: chatOllama_0-input-cache-BaseCache
          display: true
      inputs:
        cache: '{{inMemoryCache_0.data.instance}}'
        baseUrl: [% ollama_service_url %]
        modelName: [% ollama_model %]
        temperature: "0.5"
        allowImageUploads: ""
        streaming: true
        jsonMode: ""
        keepAlive: 10m
        topP: "0.5"
        topK: "25"
        mirostat: ""
        mirostatEta: ""
        mirostatTau: ""
        numCtx: "8192"
        numGpu: ""
        numThread: ""
        repeatLastN: ""
        repeatPenalty: ""
        stop: ""
        tfsZ: ""
      outputAnchors:
        - id: chatOllama_0-output-chatOllama-ChatOllama|ChatOllama|BaseChatModel|BaseLanguageModel|Runnable
          name: chatOllama
          label: ChatOllama
          description: Chat completion using open-source LLM on Ollama
          type: ChatOllama | ChatOllama | BaseChatModel | BaseLanguageModel | Runnable
      outputs: {}
      selected: false
    width: 300
    height: 741
    selected: false
    positionAbsolute:
      x: 2300
      y: 400
    dragging: false
  - id: inMemoryCache_0
    position:
      x: 1925
      y: 365
    type: customNode
    data:
      id: inMemoryCache_0
      label: InMemory Cache
      version: 1
      name: inMemoryCache
      type: InMemoryCache
      baseClasses:
        - InMemoryCache
        - BaseCache
      category: Cache
      description: Cache LLM response in memory, will be cleared once app restarted
      inputParams: []
      inputAnchors: []
      inputs: {}
      outputAnchors:
        - id: inMemoryCache_0-output-inMemoryCache-InMemoryCache|BaseCache
          name: inMemoryCache
          label: InMemoryCache
          description: Cache LLM response in memory, will be cleared once app restarted
          type: InMemoryCache | BaseCache
      outputs: {}
      selected: false
    width: 300
    height: 159
    selected: false
    positionAbsolute:
      x: 1925
      y: 365
    dragging: false
  - id: documentStoreVS_0
    position:
      x: 1925
      y: 850
    type: customNode
    data:
      id: documentStoreVS_0
      label: Document Store (Vector)
      version: 1
      name: documentStoreVS
      type: DocumentStoreVS
      baseClasses:
        - DocumentStoreVS
      category: Vector Stores
      description: Search and retrieve documents from Document Store
      inputParams:
        - label: Select Store
          name: selectedStore
          type: asyncOptions
          loadMethod: listStores
          id: documentStoreVS_0-input-selectedStore-asyncOptions
          display: true
      inputAnchors: []
      inputs:
        selectedStore: [% doc_store_id %]
      outputAnchors:
        - name: output
          label: Output
          type: options
          description: ""
          options:
            - id: documentStoreVS_0-output-retriever-BaseRetriever
              name: retriever
              label: Retriever
              description: ""
              type: BaseRetriever
            - id: documentStoreVS_0-output-vectorStore-VectorStore
              name: vectorStore
              label: Vector Store
              description: ""
              type: VectorStore
          default: retriever
      outputs:
        output: retriever
      selected: false
    width: 300
    height: 342
    selected: false
    positionAbsolute:
      x: 1925
      y: 850
    dragging: false
  - id: bufferMemory_0
    position:
      x: 1925
      y: 550
    type: customNode
    data:
      id: bufferMemory_0
      label: Buffer Memory
      version: 2
      name: bufferMemory
      type: BufferMemory
      baseClasses:
        - BufferMemory
        - BaseChatMemory
        - BaseMemory
      category: Memory
      description: Retrieve chat messages stored in database
      inputParams:
        - label: Session Id
          name: sessionId
          type: string
          description: If not specified, a random id will be used. Learn <a target="_blank" href="https://docs.flowiseai.com/memory#ui-and-embedded-chat">more</a>
          default: ""
          additionalParams: true
          optional: true
          id: bufferMemory_0-input-sessionId-string
          display: true
        - label: Memory Key
          name: memoryKey
          type: string
          default: chat_history
          additionalParams: true
          id: bufferMemory_0-input-memoryKey-string
          display: true
      inputAnchors: []
      inputs:
        sessionId: ""
        memoryKey: chat_history
      outputAnchors:
        - id: bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory
          name: bufferMemory
          label: BufferMemory
          description: Retrieve chat messages stored in database
          type: BufferMemory | BaseChatMemory | BaseMemory
      outputs: {}
      selected: false
    width: 300
    height: 279
    selected: false
    positionAbsolute:
      x: 1925
      y: 550
    dragging: false
edges:
  - source: chatOllama_0
    sourceHandle: chatOllama_0-output-chatOllama-ChatOllama|ChatOllama|BaseChatModel|BaseLanguageModel|Runnable
    target: conversationalRetrievalQAChain_0
    targetHandle: conversationalRetrievalQAChain_0-input-model-BaseChatModel
    type: buttonedge
    id: chatOllama_0-chatOllama_0-output-chatOllama-ChatOllama|ChatOllama|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel
  - source: inMemoryCache_0
    sourceHandle: inMemoryCache_0-output-inMemoryCache-InMemoryCache|BaseCache
    target: chatOllama_0
    targetHandle: chatOllama_0-input-cache-BaseCache
    type: buttonedge
    id: inMemoryCache_0-inMemoryCache_0-output-inMemoryCache-InMemoryCache|BaseCache-chatOllama_0-chatOllama_0-input-cache-BaseCache
  - source: documentStoreVS_0
    sourceHandle: documentStoreVS_0-output-retriever-BaseRetriever
    target: conversationalRetrievalQAChain_0
    targetHandle: conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever
    type: buttonedge
    id: documentStoreVS_0-documentStoreVS_0-output-retriever-BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever
  - source: bufferMemory_0
    sourceHandle: bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory
    target: conversationalRetrievalQAChain_0
    targetHandle: conversationalRetrievalQAChain_0-input-memory-BaseMemory
    type: buttonedge
    id: bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-memory-BaseMemory
