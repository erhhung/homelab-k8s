# VS Code thinks "cluster.yaml" is RKE cluster configuration, so explicitly set schema:
# yaml-language-server: $schema=https://raw.githubusercontent.com/ansible/ansible-lint/main/src/ansiblelint/schemas/ansible.json
---
# https://docs.rke2.io/install/quickstart
- name: Perform RKE2 role
  tags: rke2
  hosts: cluster
  become: true
  vars_files:
    - vars/basics.yml
    - vars/kubernetes.yml
    - vars/lablabs.rke2.yml
    - vars/vclusters.yml
  # https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_reuse_roles.html#using-roles
  pre_tasks:
    - name: Include vars/lablabs.rke2ha.yml
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/include_vars_module.html
      ansible.builtin.include_vars:
        file: vars/lablabs.rke2ha.yml
      when: rke_ha_mode

    - name: Ensure odd control plane nodes
      vars:
        count: "{{ groups[rke_control_plane_group] | length }}"
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/assert_module.html
      ansible.builtin.assert:
        that:
          - count | int is odd
        fail_msg: Number of control plane nodes must be odd!
        success_msg: Deploying {{ count }} control plane nodes.
  tasks:
    # https://github.com/rancherfederal/rke2-ansible
    # - name: Include rancherfederal.rke2.rke2 role
    #   ansible.builtin.include_role:
    #     name: rancherfederal.rke2.rke2

    # https://github.com/lablabs/ansible-role-rke2
    - name: Include lablabs.rke2 role
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/include_role_module.html
      ansible.builtin.include_role:
        name: lablabs.rke2
  any_errors_fatal: true

- name: Set up shell environment for kubectl
  tags: kubeconfig
  hosts: "{{ rke_control_plane_group }}"
  gather_facts: false
  vars_files: &vars_files
    - vars/basics.yml
    - vars/kubernetes.yml
  tasks:
    - name: Make RKE kubeconfig readable
      vars:
        file_desc: RKE kubeconfig
        file_path: "{{ rke_kubeconfig }}"
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/include_tasks_module.html
      ansible.builtin.include_tasks: tasks/fs/readable.yml

    - name: Fetch kubeconfig file content
      run_once: true
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/command_module.html
      ansible.builtin.command: cat {{ rke_kubeconfig }}
      when: file_check.stat.exists
      register: cat_kubeconfig
      changed_when: false

    - name: Merge kubeconfig into local
      vars:
        kubeconfig: "{{ rke_kubeconfig }}"
        context: "{{ rke_cluster_name }}"
        server: https://{{ rke_fqdns[0] }}:6443
      ansible.builtin.include_tasks: tasks/k8s/kubeconfig.yml
  any_errors_fatal: true

- name: Set up shell environment for kubectl
  tags: kubectl
  hosts: cluster
  gather_facts: false
  become: true
  vars_files: *vars_files
  vars:
    kubeconfig_content: "{{ hostvars[rke_control_plane_host]['cat_kubeconfig'].stdout }}"
  tasks:
    # kubectl should already be executable,
    # but not parent dirs /var/lib/rancher
    - name: Make kubectl binary executable
      vars:
        file_desc: RKE kubectl
        file_path: "{{ rke_bin_dir }}/kubectl"
        file_mode: "0755"
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/include_tasks_module.html
      ansible.builtin.include_tasks: tasks/fs/readable.yml

    - name: Write kubeconfig on worker node
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/copy_module.html
      ansible.builtin.copy:
        dest: "{{ rke_kubeconfig }}"
        content: "{{ kubeconfig_content }}"
        mode: "0644"
      when: >-
        kubeconfig_content != '' and
        inventory_hostname in groups[rke_workers_group]

    - name: Create /etc/profile.d/kubernetes.sh
      ansible.builtin.copy:
        dest: /etc/profile.d/kubernetes.sh
        content: |
          # kubectl and crictl are in the same directory
          export PATH="$PATH:{{ rke_bin_dir }}"
          export KUBECONFIG={{ rke_kubeconfig }}
          export CRI_CONFIG_FILE={{ rke_crictl_config }}
          export CONTAINER_RUNTIME_ENDPOINT={{ containerd_socket }}
        mode: "0644"

    - name: Allow admin user to use crictl
      vars:
        file_path: "{{ item.path }}"
        file_mode: "{{ item.mode }}"
      ansible.builtin.include_tasks: tasks/fs/readable.yml
      loop:
        - path: "{{ rke_crictl_config }}"
        - path: "{{ containerd_socket }}"
          mode: "0666"

    - name: Install calicoctl CLI utility
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/shell_module.html
      ansible.builtin.shell: |
        # run Bash and source /etc/profile.d
        # scripts so that kubectl is in PATH
        exec /bin/bash -l <<'EOF'
        set -o pipefail

        # get exact version of Calico deployed by RKE2 as part
        # of the Canal CNI in order to install matching client
        VER=$(kubectl get ds rke2-canal -n kube-system -o json | \
          jq -r '.spec.template.spec.containers[] |
            select(.name == "calico-node").image' | \
            sed -E 's/^.+:(v[0-9.]+).*$/\1/')

        # check if matching version already installed
        BIN=$(command -v calicoctl) &> /dev/null && {
          ver=$(calicoctl version | awk '/Client Version/ {print $3}')
          [ "$ver" == "$VER" ] && exit 9 # no change
          BIN=$(dirname "$BIN")
        } || BIN=/usr/local/bin

        ARCH=$(uname -m | sed -e 's/aarch64/arm64/' \
                              -e  's/x86_64/amd64/')
        REL="https://github.com/projectcalico/calico/releases/download"
        curl -fsSLo "$BIN/calicoctl" "$REL/$VER/calicoctl-linux-$ARCH"
        chmod +x    "$BIN/calicoctl"
        EOF
      register: calicoctl
      changed_when: calicoctl.rc == 0
      failed_when: >-
        calicoctl.rc != 0 and
        calicoctl.rc != 9
  any_errors_fatal: true

- name: Apply supplemental RKE2 configuration
  tags: configure
  hosts: cluster
  vars_files:
    - vars/basics.yml
    - vars/kubernetes.yml
    - vars/lablabs.rke2.yml
    - vars/rancher.yml
  tasks:
    # increase kube-proxy livenessProbe failureThreshold
    # since it's causing frequent unhealthy pod restarts.
    # turns out not necessary as it's a Linux kernel bug:
    # https://bugs.launchpad.net/ubuntu/+source/linux/+bug/2104282
    # - name: Patch kube-proxy livenessProbe
    #   become: true
    #   vars:
    #     failure_threshold: 12
    #   ansible.builtin.shell: |
    #     set -o pipefail
    #     cd /var/lib/rancher/rke2/agent/pod-manifests
    #
    #     yq_expr='.spec.containers[0].livenessProbe.failureThreshold = {{ failure_threshold }}'
    #     diff=$( diff <(yq -PM . kube-proxy.yaml) <(yq -PM "$yq_expr" kube-proxy.yaml) )
    #     [ "$diff" ] || exit 9 # no change
    #
    #     yq -iPM "$yq_expr" kube-proxy.yaml
    #   args:
    #     executable: /bin/bash
    #   register: patch_pod
    #   changed_when: patch_pod.rc == 0
    #   failed_when: >-
    #     patch_pod.rc != 0 and
    #     patch_pod.rc != 9
    #   notify: Restart kube-proxy static pod

    # not sure exactly why kube-proxy doesn't already have
    # permissions to watch Service, EndpointSlice and Node
    - name: Update kube-proxy ClusterRole
      become: true
      ansible.builtin.copy:
        # https://docs.rke2.io/advanced#auto-deploying-manifests
        dest: "{{ rke_manifests_dir }}/kube-proxy-clusterrole.yaml"
        content: |
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: system:kube-proxy
          rules:
            - apiGroups: [""]
              resources: ["services", "endpoints", "nodes"]
              verbs: ["get", "list", "watch"]
            - apiGroups: ["discovery.k8s.io"]
              resources: ["endpointslices"]
              verbs: ["get", "list", "watch"]
        mode: "0644"
      when: inventory_hostname == rke_control_plane_host
      register: kube_proxy_cr

    - name: Create Nginx ingress secret
      vars:
        kubeconfig: "{{ rke_kubeconfig }}"
        cert_desc: Nginx ingress
        cert_file: nginx
        secret_name: "{{ nginx_default_tls_secret }}"
        secret_ns: kube-system
        create_ns: false
      ansible.builtin.include_tasks: tasks/k8s/secrets/tls.yml
      when: inventory_hostname == rke_control_plane_host

    # https://docs.rke2.io/advanced#auto-deploying-manifests
    - name: Write HelmChartConfig manifests
      become: true
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/template_module.html
      ansible.builtin.template:
        src: "{{ item.src }}"
        dest: "{{ rke_manifests_dir }}/{{ item.dest }}"
        mode: "0664"
      loop:
        - src: templates/coredns.yaml.j2
          dest: rke2-coredns-config.yaml
        - src: templates/nginx.yaml.j2
          dest: rke2-ingress-nginx-config.yaml
      loop_control:
        label: "{{ item.dest }}"
      when: inventory_hostname == rke_control_plane_host
      register: chart_configs

    - name: Restart rke2-server service
      become: true
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/service_module.html
      ansible.builtin.service:
        name: rke2-server
        state: restarted
      # noqa no-handler
      when: >-
        kube_proxy_cr is defined and
        kube_proxy_cr.changed    or
        chart_configs is defined and
        chart_configs.results  | map(attribute='changed') is any
      register: restart_rke

    # whenever the rke2-server service is restarted,
    # kubeconfig is rewritten with 0600 permissions
    - name: Make RKE kubeconfig readable
      vars:
        file_desc: RKE kubeconfig
        file_path: "{{ rke_kubeconfig }}"
      ansible.builtin.include_tasks: tasks/fs/readable.yml
      when: >-
        restart_rke is defined and
        restart_rke.changed

    - name: Register cluster with Rancher
      vars:
        # required kubernetes>=24.2 package only in user virtualenv
        ansible_python_interpreter: "{{ venv_python_interpreter }}"
      when: inventory_hostname == rke_control_plane_host
      block:
        - name: Wait for CoreDNS to be ready
          # https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_info_module.html
          kubernetes.core.k8s_info:
            kubeconfig: "{{ rke_kubeconfig }}"
            api_version: apps/v1
            kind: Deployment
            name: rke2-coredns-rke2-coredns
            namespace: kube-system
          # https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_loops.html#retrying-a-task-until-a-condition-is-met
          register: coredns_info
          until: >-
            coredns_info.resources[0] is defined and
            coredns_info.resources[0].status.readyReplicas ==
            coredns_info.resources[0].status.replicas
          retries: 30
          delay: 10

        - name: Apply worker role node label
          # this could also have been done using "k8s_node_label" config variable of RKE2 role
          # https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html
          kubernetes.core.k8s:
            kubeconfig: "{{ rke_kubeconfig }}"
            api_version: v1
            kind: Node
            name: "{{ item }}"
            definition:
              metadata:
                labels:
                  node-role.kubernetes.io/worker: "true"
            state: patched
            wait: true
          # all cluster nodes, including control plane node(s),
          # can run user workloads, so they are labeled worker
          # role (control plane nodes have already been labeled
          # with roles: "control-plane", "etcd", and "master")
          loop: "{{ groups['cluster'] }}"

        - name: Apply registration manifest
          vars:
            manifest_url: >-
              {{ hostvars['rancher']['rke_reg_url'] if 'rke_reg_url' in
                 hostvars['rancher'] }}
          kubernetes.core.k8s:
            kubeconfig: "{{ rke_kubeconfig }}"
            src: "{{ manifest_url }}"
            state: present
            apply: true
            wait: true
            wait_timeout: 10m0s
          timeout: 1800
          when: manifest_url

        - name: Wait for cluster agent be ready
          kubernetes.core.k8s_info:
            kubeconfig: "{{ rke_kubeconfig }}"
            api_version: apps/v1
            kind: Deployment
            name: cattle-cluster-agent
            namespace: "{{ rancher_namespace }}"
          register: agent_info
          until: >-
            agent_info.resources[0] is defined and
            agent_info.resources[0].status.readyReplicas ==
            agent_info.resources[0].status.replicas
          retries: 30
          delay: 10
  # handlers:
  #   # restart a static pod by deleting it
  #   - name: Restart kube-proxy static pod
  #     vars:
  #       # required kubernetes>=24.2 package only in user virtualenv
  #       ansible_python_interpreter: "{{ venv_python_interpreter }}"
  #     kubernetes.core.k8s:
  #       kubeconfig: "{{ rke_kubeconfig }}"
  #       api_version: v1
  #       kind: Pod
  #       name: kube-proxy-{{ inventory_hostname }}
  #       namespace: kube-system
  #       state: absent
  any_errors_fatal: true
