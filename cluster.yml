# VS Code thinks "cluster.yaml" is RKE cluster configuration, so explicitly set schema:
# yaml-language-server: $schema=https://raw.githubusercontent.com/ansible/ansible-lint/main/src/ansiblelint/schemas/ansible.json
---
# https://docs.rke2.io/install/quickstart
- name: Perform RKE2 role
  tags: rke2
  hosts: cluster
  become: true
  vars_files:
    - vars/kubernetes.yml
    - vars/lablabs.rke2.yml
    - vars/vclusters.yml
  # https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_reuse_roles.html#using-roles
  pre_tasks:
    - name: Include vars/lablabs.rke2ha.yml
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/include_vars_module.html
      ansible.builtin.include_vars: vars/lablabs.rke2ha.yml
      when: rke_ha_mode

    - name: Ensure odd control plane nodes
      vars:
        count: "{{ groups[rke_control_plane_group] | length }}"
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/assert_module.html
      ansible.builtin.assert:
        that:
          - count | int is odd
        fail_msg: Number of control plane nodes must be odd!
        success_msg: Deploying {{ count }} control plane nodes.
  tasks:
    # https://github.com/lablabs/ansible-role-rke2
    - name: Include lablabs.rke2 role
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/include_role_module.html
      ansible.builtin.include_role:
        name: lablabs.rke2

      # NOTE: once lots of workloads are running in the cluster, and config change
      # triggers the task "lablabs.rke2 : Restart RKE2 service on <node>", Ansible
      # may fail with error "Unable to restart service rke2-server.service" due to
      # timeout. Simply restart the service (and likely etcd pod) manually on each
      # node (systemctl restart rke2-server), wait for kube-system pods to settle,
      # and then re-run this playbook.
  any_errors_fatal: true

- name: Set up shell environment for kubectl
  tags: kubeconfig
  hosts: "{{ rke_control_plane_group }}"
  gather_facts: false
  vars_files: &vars_files
    - vars/kubernetes.yml
  tasks:
    - name: Make RKE kubeconfig readable
      vars:
        file_desc: RKE kubeconfig
        file_path: "{{ rke_kubeconfig }}"
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/include_tasks_module.html
      ansible.builtin.include_tasks: tasks/fs/readable.yml

    - name: Fetch kubeconfig file content
      run_once: true
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/command_module.html
      ansible.builtin.command: cat {{ rke_kubeconfig }}
      when: file_check.stat.exists
      register: cat_kubeconfig
      changed_when: false

    - name: Merge kubeconfig into local
      run_once: true
      vars:
        kubeconfig: "{{ rke_kubeconfig }}"
        context: "{{ rke_cluster_name }}"
        # rke_fqdns[0] resolves to the virtual IP, but,
        # for reasons not yet known, the connection is
        # very flaky, while resolving directly to k8s1
        # or another control plane node is very stable
        server: https://{{ rke_fqdns[1] }}:6443
      ansible.builtin.include_tasks: tasks/k8s/kubeconfig.yml
  any_errors_fatal: true

- name: Set up shell environment for kubectl
  tags: kubectl
  hosts: cluster
  gather_facts: false
  become: true
  vars_files: *vars_files
  vars:
    kubeconfig_content: "{{ hostvars[rke_control_plane_host]['cat_kubeconfig'].stdout }}"
  tasks:
    # kubectl should already be executable,
    # but not parent dirs /var/lib/rancher
    - name: Make kubectl binary executable
      vars:
        file_desc: RKE kubectl
        file_path: "{{ rke_bin_dir }}/kubectl"
        file_mode: "0755"
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/include_tasks_module.html
      ansible.builtin.include_tasks: tasks/fs/readable.yml

    - name: Write kubeconfig on worker node
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/copy_module.html
      ansible.builtin.copy:
        dest: "{{ rke_kubeconfig }}"
        content: "{{ kubeconfig_content }}"
        mode: "0644"
      when: >-
        kubeconfig_content != '' and
        inventory_hostname in groups[rke_workers_group]

    - name: Create /etc/profile.d/kubernetes.sh
      ansible.builtin.copy:
        dest: /etc/profile.d/kubernetes.sh
        content: |
          # kubectl and crictl are in the same directory
          export PATH="$PATH:{{ rke_bin_dir }}"
          export KUBECONFIG={{ rke_kubeconfig }}
          export CRI_CONFIG_FILE={{ rke_crictl_config }}
          export CONTAINER_RUNTIME_ENDPOINT={{ containerd_socket }}
        mode: "0644"

    - name: Allow admin user to use crictl
      vars:
        file_path: "{{ item.path }}"
        file_mode: "{{ item.mode }}"
      ansible.builtin.include_tasks: tasks/fs/readable.yml
      loop:
        - path: "{{ rke_crictl_config }}"
        - path: "{{ containerd_socket }}"
          mode: "0666"

    - name: Install calicoctl CLI utility
      # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/shell_module.html
      ansible.builtin.shell: |
        # run Bash and source /etc/profile.d
        # scripts so that kubectl is in PATH
        exec /bin/bash -l <<'EOT'
        set -o pipefail

        # get exact version of Calico deployed by RKE2 as part
        # of the Canal CNI in order to install matching client
        VER=$(kubectl get ds rke2-canal -n kube-system -o json | \
          jq -r '.spec.template.spec.containers[] |
            select(.name == "calico-node").image' | \
            sed -E 's/^.+:(v[0-9.]+).*$/\1/')

        # check if matching version already installed
        BIN=$(command -v calicoctl) &> /dev/null && {
          ver=$(calicoctl version | awk '/Client Version/ {print $3}')
          [ "$ver" == "$VER" ] && exit 9 # no change
          BIN=$(dirname "$BIN")
        } || BIN=/usr/local/bin

        ARCH=$(uname -m | sed -e 's/aarch64/arm64/' \
                              -e  's/x86_64/amd64/')
        REL="https://github.com/projectcalico/calico/releases/download"
        curl -fsSLo "$BIN/calicoctl" "$REL/$VER/calicoctl-linux-$ARCH"
        chmod +x    "$BIN/calicoctl"
        EOT
      register: install_cli
      changed_when: install_cli.rc == 0
      failed_when: >-
        install_cli.rc != 0 and
        install_cli.rc != 9
  any_errors_fatal: true

- name: Apply supplemental RKE2 configuration
  tags: configure
  hosts: cluster
  vars_files:
    - vars/basics.yml
    - vars/kubernetes.yml
    - vars/lablabs.rke2.yml
    - vars/rancher.yml
  vars:
    rke2_svc: "rke2-{{ 'server' if inventory_hostname
      in groups[rke_control_plane_group] else 'agent' }}"
  tasks:
    # https://github.com/containerd/cri/tree/master/docs/config.md
    # search "base_runtime_spec" setting with "cri-base.json" (note that
    # "io.containerd.grpc.v1.cri" is now "io.containerd.cri.v1.runtime")
    - name: Set containerd RLIMIT_NOFILE
      become: true
      vars:
        containerd_config_dir: "{{ containerd_config | dirname }}"
      block:
        - name: Write containerd/config.toml.tmpl
          # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/template_module.html
          ansible.builtin.template:
            src: "{{ template_dir }}/rke2/containerd/config.toml.tmpl.j2"
            # https://docs.rke2.io/advanced#configuring-containerd
            dest: "{{ containerd_config_dir }}/config-v3.toml.tmpl"
            mode: "0644"
          register: containerd_toml

        # generate base OCI spec JSON by installing
        # containerd package; running: ctr oci spec
        - name: Write containerd/cri-base.json
          vars:
            nofile_limit: "{{ pam_limits[0].value }}"
          ansible.builtin.template:
            src: "{{ template_dir }}/rke2/containerd/cri-base.json.j2"
            dest: "{{ containerd_config_dir }}/cri-base.json"
            mode: "0644"
          register: cri_base_json

    # increase kube-proxy livenessProbe failureThreshold
    # since it's causing frequent unhealthy pod restarts.
    # turns out not necessary as it's a Linux kernel bug:
    # https://bugs.launchpad.net/ubuntu/+source/linux/+bug/2104282
    # - name: Patch kube-proxy livenessProbe
    #   become: true
    #   vars:
    #     failure_threshold: 12
    #   ansible.builtin.shell: |
    #     set -o pipefail
    #     cd /var/lib/rancher/rke2/agent/pod-manifests
    #
    #     yq_expr='.spec.containers[0].livenessProbe.failureThreshold = {{ failure_threshold }}'
    #     diff=$( diff <(yq -PM . kube-proxy.yaml) <(yq -PM "$yq_expr" kube-proxy.yaml) )
    #     [ "$diff" ] || exit 9 # no change
    #
    #     yq -iPM "$yq_expr" kube-proxy.yaml
    #   args:
    #     executable: /bin/bash
    #   register: patch_pod
    #   changed_when: patch_pod.rc == 0
    #   failed_when: >-
    #     patch_pod.rc != 0 and
    #     patch_pod.rc != 9
    #   notify: Restart kube-proxy static pod

    # not sure exactly why kube-proxy doesn't already have
    # permissions to watch Service, EndpointSlice and Node
    - name: Update kube-proxy ClusterRole
      become: true
      ansible.builtin.copy:
        # https://docs.rke2.io/advanced#auto-deploying-manifests
        dest: "{{ rke_manifests_dir }}/kube-proxy-clusterrole.yaml"
        content: |
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: system:kube-proxy
          rules:
            - apiGroups: [""]
              resources: ["services", "endpoints", "nodes"]
              verbs: ["get", "list", "watch"]
            - apiGroups: ["discovery.k8s.io"]
              resources: ["endpointslices"]
              verbs: ["get", "list", "watch"]
        mode: "0644"
      when: inventory_hostname == rke_control_plane_host
      register: kube_proxy_cr

    - name: Create Nginx ingress secret
      vars:
        kubeconfig: "{{ rke_kubeconfig }}"
        cert_desc: Nginx ingress
        cert_file: nginx
        secret_name: "{{ nginx_default_tls_secret }}"
        secret_ns: kube-system
        create_ns: false
      ansible.builtin.include_tasks: tasks/k8s/secrets/tls.pki.yml
      when: inventory_hostname == rke_control_plane_host

    # https://docs.rke2.io/advanced#auto-deploying-manifests
    - name: Write HelmChartConfig manifests
      become: true
      ansible.builtin.template:
        src: "{{ template_dir }}/{{ item.src }}"
        dest: "{{ rke_manifests_dir }}/{{ item.dest }}"
        mode: "0664"
      loop:
        - src: rke2/coredns.yaml.j2
          dest: rke2-coredns-config.yaml
        - src: rke2/nginx.yaml.j2
          dest: rke2-ingress-nginx-config.yaml
      loop_control:
        label: "{{ item.dest }}"
      when: inventory_hostname == rke_control_plane_host
      register: chart_configs

    # this task is known to fail for reasons not yet known,
    # and manually restarting the rke2-server service also
    # often doesn't work, but rebooting the node does work,
    # followed by continuing from the rancher.yml playbook
    - name: Restart {{ rke2_svc }} service
      # noqa no-handler
      when: >-
        containerd_toml.changed   or
        cri_base_json.changed     or
        kube_proxy_cr is defined and
        kube_proxy_cr.changed     or
        chart_configs is defined and
        chart_configs.results  | map(attribute='changed') is any
      become: true
      block:
        - name: Stop {{ rke2_svc }} service
          # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/systemd_service_module.html
          ansible.builtin.systemd_service:
            name: "{{ rke2_svc }}"
            state: stopped

        - name: Wait until processes exit
          # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/pause_module.html
          ansible.builtin.pause:
            seconds: 60

        - name: Start {{ rke2_svc }} service
          ansible.builtin.systemd_service:
            name: "{{ rke2_svc }}"
            state: started
          timeout: 300
          register: restart_rke
          until: >-
            restart_rke.state is  defined and
            restart_rke.state == 'started'
          retries: 3
          delay: 10

        - name: Check {{ rke2_svc }} status
          # noqa command-instead-of-module
          ansible.builtin.command: systemctl is-active {{ rke2_svc }}
          changed_when: false
          register: is_active
          until: is_active.stdout | trim == 'active'
          retries: 30
          delay: 10

    # whenever the rke2-server service is restarted,
    # kubeconfig is rewritten with 0600 permissions
    - name: Make RKE kubeconfig readable
      vars:
        file_desc: RKE kubeconfig
        file_path: "{{ rke_kubeconfig }}"
      ansible.builtin.include_tasks: tasks/fs/readable.yml
      when: >-
        restart_rke is defined and
        restart_rke.changed

    - name: Register cluster with Rancher
      when: inventory_hostname == rke_control_plane_host
      vars:
        # required kubernetes>=24.2 package only in user virtualenv
        ansible_python_interpreter: "{{ venv_python_interpreter }}"
      block:
        - name: Wait for CoreDNS to be ready
          # https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_info_module.html
          kubernetes.core.k8s_info:
            kubeconfig: "{{ rke_kubeconfig }}"
            api_version: apps/v1
            kind: Deployment
            name: rke2-coredns-rke2-coredns
            namespace: kube-system
          # https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_loops.html#retrying-a-task-until-a-condition-is-met
          register: deploy_info
          until: >-
            deploy_info.resources[0].status               is defined and
            deploy_info.resources[0].status.readyReplicas is defined and
            deploy_info.resources[0].status.readyReplicas ==
            deploy_info.resources[0].status.replicas
          retries: 30
          delay: 10

        - name: Apply additional node labels
          vars:
            labels_by_node: |
              {% set nodes = {} %}
              {% for label in additional_node_labels %}
              {%   for node in label.nodes %}
              {%     set _ = nodes.setdefault(node, {}).update({label.label: label.value}) %}
              {%   endfor %}
              {% endfor %}
              {{ nodes  }}
          # https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html
          kubernetes.core.k8s:
            kubeconfig: "{{ rke_kubeconfig }}"
            api_version: v1
            kind: Node
            name: "{{ item }}"
            definition:
              metadata:
                labels: "{{ labels_by_node[item] }}"
            state: patched
          loop: "{{ groups['cluster'] }}"
          when: item in labels_by_node

        - name: Apply registration manifest
          vars:
            manifest_url: >-
              {{ hostvars['rancher']['rke_reg_url'] if 'rke_reg_url' in
                 hostvars['rancher'] }}
          kubernetes.core.k8s:
            kubeconfig: "{{ rke_kubeconfig }}"
            src: "{{ manifest_url }}"
            state: present
            apply: true
            wait: true
            wait_timeout: 600 # requires wait=true
          timeout: 1800
          when: manifest_url

        - name: Wait for agent to be ready
          kubernetes.core.k8s_info:
            kubeconfig: "{{ rke_kubeconfig }}"
            api_version: apps/v1
            kind: Deployment
            name: cattle-cluster-agent
            namespace: "{{ rancher_namespace }}"
          register: deploy_info
          until: >-
            deploy_info.resources[0].status               is defined and
            deploy_info.resources[0].status.readyReplicas is defined and
            deploy_info.resources[0].status.readyReplicas ==
            deploy_info.resources[0].status.replicas
          retries: 30
          delay: 10
  # handlers:
  #   # restart a static pod by deleting it
  #   - name: Restart kube-proxy static pod
  #     vars:
  #       # required kubernetes>=24.2 package only in user virtualenv
  #       ansible_python_interpreter: "{{ venv_python_interpreter }}"
  #     kubernetes.core.k8s:
  #       kubeconfig: "{{ rke_kubeconfig }}"
  #       api_version: v1
  #       kind: Pod
  #       name: kube-proxy-{{ inventory_hostname }}
  #       namespace: kube-system
  #       state: absent
  any_errors_fatal: true
